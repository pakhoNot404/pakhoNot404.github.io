<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tramac&#39;s Space</title>
  
  <subtitle>Keep trying</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tramac.github.io/"/>
  <updated>2021-03-26T10:18:18.337Z</updated>
  <id>http://tramac.github.io/</id>
  
  <author>
    <name>Tramac</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>隐秘的角落</title>
    <link href="http://tramac.github.io/2021/03/26/hidden-corner/"/>
    <id>http://tramac.github.io/2021/03/26/hidden-corner/</id>
    <published>2021-03-26T09:05:39.000Z</published>
    <updated>2021-03-26T10:18:18.337Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为什么会有这个东西"><a href="#为什么会有这个东西" class="headerlink" title="为什么会有这个东西?"></a>为什么会有这个东西?</h3><p>工作之后，很少有时间去维护github上项目了，但是又不忍心忽略掉小伙伴的issues，为了能够更快的解决小伙伴的问题，所以决定将我的微信放在这里！如果遇到以下问题就加我好友吧：</p><ul><li>使用项目期间发现有解决不掉的bug</li><li>觉得项目有缺陷，你有一些改进建议</li><li>你的科研&amp;工作期间有一些问题想和我交流一下</li><li>或者你有一些奇奇怪怪的想法</li></ul><p align="center"><img width="30%" src="/images/Wechat.jpg" /></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;为什么会有这个东西&quot;&gt;&lt;a href=&quot;#为什么会有这个东西&quot; class=&quot;headerlink&quot; title=&quot;为什么会有这个东西?&quot;&gt;&lt;/a&gt;为什么会有这个东西?&lt;/h3&gt;&lt;p&gt;工作之后，很少有时间去维护github上项目了，但是又不忍心忽略掉小伙伴的iss
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>learning-to-bert</title>
    <link href="http://tramac.github.io/2020/12/11/learning-to-bert/"/>
    <id>http://tramac.github.io/2020/12/11/learning-to-bert/</id>
    <published>2020-12-11T07:23:49.000Z</published>
    <updated>2020-12-13T01:46:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>在一些视频理解任务中，比如分类，VQA等，除了画面信息之外通常还会引入一些音频、文本等信息。为了使视频任务中引入标题信息（无论是否有帮助，但总要尝试），决定利用BERT来提取标题向量。</p><p>看了google官方的<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">bert</a>以及huggingface所实现的pytorch版本的<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a>，对于一个NLP小白来说可能是由于bert的变种太多实在太过眼花缭乱，所以本文的目标就是得到一个可提取中文文本特征向量的接口。</p><p>对我而言，接触一个任务时习惯性的会考虑三个问题：数据、模型以及损失函数。这篇Blog也根据该思路来对BERT做一个学习。</p><h2 id="BERT的输入是什么？"><a href="#BERT的输入是什么？" class="headerlink" title="BERT的输入是什么？"></a>BERT的输入是什么？</h2><p>该部分直接参考官方的<a href="https://github.com/google-research/bert/blob/master/tokenization.py" target="_blank" rel="noopener"><code>tokenization.py</code></a>来说明是如何对输入进行预处理的，其它框架虽然略有不同，但也都是由此演变而来。<code>tokenization.py</code>中包含了两个分词器：<code>BasicTokenizer</code>和<code>WordpieceTokenizer</code>，下面我们根据例子来说明是如何处理的。</p><p>给定一段文本：<code>text=“新垣结衣（Aragaki Yui），1988年6月11日出生于日本冲绳县那霸市。日本女演员、歌手、模特\r\n”。</code></p><h3 id="BasicTokenizer"><a href="#BasicTokenizer" class="headerlink" title="BasicTokenizer"></a>BasicTokenizer</h3><p><code>BasicTokenizer</code>是一个初步分词器。对于一个输入文本，处理流程为：转unicode-&gt;去除特殊字符-&gt;处理中文-&gt;按空格分词-&gt;去除多余字符和标点分词-&gt;再次按空格分词。</p><h4 id="转unicode编码"><a href="#转unicode编码" class="headerlink" title="转unicode编码"></a>转unicode编码</h4><p>具体可以看<code>convert_to_unicode(text)</code>函数，大概流程是：对python3而言，输入为<code>str</code>类型时直接返回，为<code>bytes</code>型时转为<code>unicode</code>类型；对python2而言，输入为<code>str</code>类型时则转为<code>unicode</code>类型，为<code>unicode</code>类型时则直接返回。</p><p>经过转码处理之后，text和原来相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>text=convert_to_unicode(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line"><span class="string">'新垣结衣（Aragaki Yui），1988年6月11日出生于日本冲绳县那霸市。日本女演员、歌手、模特\r\n'</span></span><br></pre></td></tr></table></figure><h4 id="清除无效字符及空白字符"><a href="#清除无效字符及空白字符" class="headerlink" title="清除无效字符及空白字符"></a>清除无效字符及空白字符</h4><p>该部分对应于<code>_clean_text(self, text)</code>函数，主要用于去除无效字符和多余空格。</p><p>源码中的<code>ord()</code>函数是以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值。</p><p>整体流程如下：</p><ul><li>码位=0时</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在一些视频理解任务中，比如分类，VQA等，除了画面信息之外通常还会引入一些音频、文本等信息。为了使视频任务中引入标题信息（无论是否有帮助，但总要尝试），决定利用BERT来提取标题向量。&lt;/p&gt;
&lt;p&gt;看了google官方的&lt;a href=&quot;https://github.co
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>learning-to-reinforcement-learning</title>
    <link href="http://tramac.github.io/2020/11/29/learning-to-reinforcement-learning/"/>
    <id>http://tramac.github.io/2020/11/29/learning-to-reinforcement-learning/</id>
    <published>2020-11-29T08:02:16.000Z</published>
    <updated>2020-12-10T13:37:42.226Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习记录，学习资料为ZhouBolei大神的<a href="https://github.com/zhoubolei/introRL" target="_blank" rel="noopener">introRL</a>。</p><h2 id="Foundation"><a href="#Foundation" class="headerlink" title="Foundation"></a>Foundation</h2><ul><li><input checked="" disabled="" type="checkbox"> Lecture1: Overview (课程概括与RL基础)<ul><li><strong>Keywords</strong>: Agent, Environment, action, reward</li><li>Supervised learning: Annotated images, data follows i.i.d distribution（条件独立同分布）</li><li>Reinforcement learning: Data are not i.i.d, a correlated time series data(数据不符合i.i.d条件，前后有时序关系); No instant feedback or label for correct action(不能马上得到反馈)</li><li>Features of RL: Trial-and-error exploration; Delayed reward; Time matters(sequential data, non i.i.d data); Agent’s actions changes the environment.</li><li><strong>Rewards</strong>: A scalar feedback signal; Indicate how well agent is doing at step t; RL is based on the maximization of rewards.</li><li>Sequential Decision Making: Trade-off between immediate reward and long-term reward.</li><li>An RL agent components: Policy, Value function, Model.</li><li>Agents type: Value-based agent, Policy-based agent, Actor-Critic agent.</li><li>Exploration and Exploitation.</li></ul></li></ul><h2 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;强化学习记录，学习资料为ZhouBolei大神的&lt;a href=&quot;https://github.com/zhoubolei/introRL&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;introRL&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;Foundation
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>talk-learning</title>
    <link href="http://tramac.github.io/2020/09/23/talk-learning/"/>
    <id>http://tramac.github.io/2020/09/23/talk-learning/</id>
    <published>2020-09-23T12:19:16.000Z</published>
    <updated>2020-10-22T03:23:12.563Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input checked="" disabled="" type="checkbox"> <p>基于视频的时序建模与动作识别&lt;<a href="https://www.techbeat.net/talk-info?id=225" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Keywords:</strong>ARTNet, TSN, UntrimmedNet</p></li><li><input checked="" disabled="" type="checkbox"> <p>Squeeze-and-Excitation Networks&lt;<a href="https://www.techbeat.net/talk-info?id=218" target="_blank" rel="noopener">Talk</a>&gt;</p></li><li><input checked="" disabled="" type="checkbox"> <p>基于光流的视频语义分割和物体检测&lt;<a href="https://www.techbeat.net/talk-info?id=212" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Keywords:</strong>Opital flow, Feature propagation(特征传播), Feature aggregation</p></li><li><input checked="" disabled="" type="checkbox"> <p>From Faster R-CNN to Mask R-CNN&lt;<a href="https://www.techbeat.net/talks/MTU5NjMzNjcxMDg4MS0xOTQtNzYwNzM=" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Key function:</strong> Classification, Location, Mask(per pixel) classification, Lanmarks location</p></li><li><input checked="" disabled="" type="checkbox"> <p>理解和利用CNN的内部表征&lt;<a href="https://www.techbeat.net/talks/MTU5NTY4OTI2NTk3NS0xODAtMjgxMDk=" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Outlines:</strong> Visualizing the internal units &amp; Weakly supervised localization and class-specific saliency<br><strong>Keywords:</strong>: 神经元响应，内部表征，物体表征，可视化，可解释性</p></li></ul><a id="more"></a><ul><li><input checked="" disabled="" type="checkbox"> <p>深度图像检索前沿技术与实际应用&lt;<a href="https://www.techbeat.net/talks/MTU5NTU5MDI2ODUxOC0xNzEtNTY0Mg==" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Defining Image Retrieval:</strong> Given an image query, generate a rank of all similar images.<br><strong>Defining instance retrieval:</strong> Given a target object of image query, generate a rank of all images containing the target object.<br><strong>Retrieval pipeline:</strong> Query image—Image Representations—Image Matching—Ranking List—Spatial Re-ranking/Query Expansion</p></li><li><input checked="" disabled="" type="checkbox"> <p>自动驾驶中的视觉技术&lt;<a href="https://www.techbeat.net/talks/MTU5NTI1NTY5NjMzMC0xNjMtNjY2OA==" target="_blank" rel="noopener">Talk</a>&gt;<br><strong>Levels of Driving:</strong> L1-L5<br><strong>Techniques(L3):</strong> High precision map/LiDAR<br><strong>Outlines:</strong> Stereo/Motion/Detection&amp;Tracking/Scene Understanding/Localization and Mapping(SLAM)</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TODO&quot;&gt;&lt;a href=&quot;#TODO&quot; class=&quot;headerlink&quot; title=&quot;TODO&quot;&gt;&lt;/a&gt;TODO&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;基于视频的时序建模与动作识别&amp;lt;&lt;a href=&quot;https://www.techbeat.net/talk-info?id=225&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talk&lt;/a&gt;&amp;gt;&lt;br&gt;&lt;strong&gt;Keywords:&lt;/strong&gt;ARTNet, TSN, UntrimmedNet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;Squeeze-and-Excitation Networks&amp;lt;&lt;a href=&quot;https://www.techbeat.net/talk-info?id=218&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talk&lt;/a&gt;&amp;gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;基于光流的视频语义分割和物体检测&amp;lt;&lt;a href=&quot;https://www.techbeat.net/talk-info?id=212&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talk&lt;/a&gt;&amp;gt;&lt;br&gt;&lt;strong&gt;Keywords:&lt;/strong&gt;Opital flow, Feature propagation(特征传播), Feature aggregation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;From Faster R-CNN to Mask R-CNN&amp;lt;&lt;a href=&quot;https://www.techbeat.net/talks/MTU5NjMzNjcxMDg4MS0xOTQtNzYwNzM=&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talk&lt;/a&gt;&amp;gt;&lt;br&gt;&lt;strong&gt;Key function:&lt;/strong&gt; Classification, Location, Mask(per pixel) classification, Lanmarks location&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;理解和利用CNN的内部表征&amp;lt;&lt;a href=&quot;https://www.techbeat.net/talks/MTU5NTY4OTI2NTk3NS0xODAtMjgxMDk=&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Talk&lt;/a&gt;&amp;gt;&lt;br&gt;&lt;strong&gt;Outlines:&lt;/strong&gt; Visualizing the internal units &amp;amp; Weakly supervised localization and class-specific saliency&lt;br&gt;&lt;strong&gt;Keywords:&lt;/strong&gt;: 神经元响应，内部表征，物体表征，可视化，可解释性&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>audio-video</title>
    <link href="http://tramac.github.io/2020/09/08/audio-video/"/>
    <id>http://tramac.github.io/2020/09/08/audio-video/</id>
    <published>2020-09-08T12:00:17.000Z</published>
    <updated>2020-09-11T07:12:06.416Z</updated>
    
    <content type="html"><![CDATA[<h2 id="视频理解中audio信息的使用"><a href="#视频理解中audio信息的使用" class="headerlink" title="视频理解中audio信息的使用"></a>视频理解中audio信息的使用</h2><h4 id="读取音频-amp-可视化波形图"><a href="#读取音频-amp-可视化波形图" class="headerlink" title="读取音频&amp;可视化波形图"></a>读取音频&amp;可视化波形图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'agg'</span>) <span class="comment"># 服务器无GUI使用matplotlib绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> soundfile <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"></span><br><span class="line">wav_data, sr = sf.read(wav_file, dtype=<span class="string">'int16'</span>)</span><br><span class="line">samples = wav_data / <span class="number">32768.0</span>  <span class="comment"># Convert to [-1.0, +1.0]</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">'Waveform'</span>)</span><br><span class="line">librosa.display.waveplot(samples, sr=sr)</span><br><span class="line">plt.savefig(<span class="string">"wav.png"</span>)</span><br></pre></td></tr></table></figure><p><img src="/Users/zhaoxiangming/Desktop/wav.png" alt="wav"></p><p>声音波形图只是把一维的时域信号直观的显示出来，并没有什么有用的信息。</p><h4 id="声谱图-spectrogram"><a href="#声谱图-spectrogram" class="headerlink" title="声谱图(spectrogram)"></a>声谱图(spectrogram)</h4><p>声谱图，简单来说就是将声音信号通过短时傅里叶变换，获得一张包含3个维度数据的热力图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#librosa.display.specshow(librosa.power_to_db(spectrogram.T, ref=np.max), sr=audio_sample_rate, x_axis='time', y_axis='linear')</span></span><br><span class="line">librosa.display.specshow(spectrogram.T, sr=audio_sample_rate, x_axis=<span class="string">'time'</span>, y_axis=<span class="string">'linear'</span>)</span><br><span class="line">plt.colorbar(format=<span class="string">'%+2.0f dB'</span>)</span><br><span class="line">plt.title(<span class="string">'spectrogram'</span>)</span><br><span class="line">plt.savefig(<span class="string">'spec.png'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;视频理解中audio信息的使用&quot;&gt;&lt;a href=&quot;#视频理解中audio信息的使用&quot; class=&quot;headerlink&quot; title=&quot;视频理解中audio信息的使用&quot;&gt;&lt;/a&gt;视频理解中audio信息的使用&lt;/h2&gt;&lt;h4 id=&quot;读取音频-amp-可视化波
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Pytorch有哪些坑？</title>
    <link href="http://tramac.github.io/2020/07/23/Pytorch%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91%EF%BC%9F/"/>
    <id>http://tramac.github.io/2020/07/23/Pytorch%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91%EF%BC%9F/</id>
    <published>2020-07-23T09:41:14.000Z</published>
    <updated>2020-07-24T06:36:03.921Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-torchvision-transform-ToTensor-中的细节"><a href="#1-torchvision-transform-ToTensor-中的细节" class="headerlink" title="1 torchvision.transform.ToTensor()中的细节"></a>1 torchvision.transform.ToTensor()中的细节</h3><p>功能：把一个取值范围是<code>[0,255]</code>的<code>PIL.Image</code>或者<code>shape</code>为<code>(H,W,C)</code>的<code>numpy.ndarray</code>,转换成形状为<code>(C,H,W)</code>，取值范围是<code>[0,1.0]</code>的<code>torch.FloatTensor</code>。</p><p>注意：只有当<code>numpy.ndarray</code>的<code>dtype=unit8</code>时才会将像素值scale到<code>[0,1.0]</code>。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">  transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line">img = Image.open(<span class="string">'cat.png'</span>) <span class="comment"># dtype=uint8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># case1: 正常情况</span></span><br><span class="line">img1 = np.asarray(img)</span><br><span class="line">img1 = transform(img1)</span><br><span class="line">img1 = img3.permute([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]) <span class="comment"># CxHxW -&gt; HxWxC，方便后续对比</span></span><br><span class="line">img1 = img1.cpu().data.numpy()</span><br><span class="line">print(img1[<span class="number">300</span>: <span class="number">305</span>, <span class="number">695</span>: <span class="number">700</span>, <span class="number">0</span>]) <span class="comment"># 打印部分像素值</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[0.43529412 0.44705883 0.45490196 0.45882353 0.45490196]</span></span><br><span class="line"><span class="string"> [0.41960785 0.43137255 0.4392157  0.44313726 0.4392157 ]</span></span><br><span class="line"><span class="string"> [0.40392157 0.41568628 0.42745098 0.43137255 0.43529412]</span></span><br><span class="line"><span class="string"> [0.3882353  0.40784314 0.41568628 0.41960785 0.42745098]</span></span><br><span class="line"><span class="string"> [0.38431373 0.4        0.4117647  0.41568628 0.42352942]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># case2: 错误操作，将unit8数据类型转换为float32，导致没有scale操作</span></span><br><span class="line">img2 = np.asarray(img, dtype=np.float32) <span class="comment"># 由于这里使得torch.ByteTensor=False，导致ToTensor()没有执行div(255)操作</span></span><br><span class="line">img2 = transform(img2)</span><br><span class="line">img2 = img2.permute([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">img2 = img2.cpu().data.numpy()</span><br><span class="line">print(img2[<span class="number">300</span>: <span class="number">305</span>, <span class="number">695</span>: <span class="number">700</span>, <span class="number">0</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[111. 114. 116. 117. 116.]</span></span><br><span class="line"><span class="string"> [107. 110. 112. 113. 112.]</span></span><br><span class="line"><span class="string"> [103. 106. 109. 110. 111.]</span></span><br><span class="line"><span class="string"> [ 99. 104. 106. 107. 109.]</span></span><br><span class="line"><span class="string"> [ 98. 102. 105. 106. 108.]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># case3: 正常情况，不使用ToTensor()执行scale操作</span></span><br><span class="line">img3 = np.asarray(img, dtype=np.float32)</span><br><span class="line">img3 = img3 / <span class="number">255.</span></span><br><span class="line">print(img3[<span class="number">300</span>: <span class="number">305</span>, <span class="number">695</span>: <span class="number">700</span>, <span class="number">0</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[0.43529412 0.44705883 0.45490196 0.45882353 0.45490196]</span></span><br><span class="line"><span class="string"> [0.41960785 0.43137255 0.4392157  0.44313726 0.4392157 ]</span></span><br><span class="line"><span class="string"> [0.40392157 0.41568628 0.42745098 0.43137255 0.43529412]</span></span><br><span class="line"><span class="string"> [0.3882353  0.40784314 0.41568628 0.41960785 0.42745098]</span></span><br><span class="line"><span class="string"> [0.38431373 0.4        0.4117647  0.41568628 0.42352942]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>结论：使用<code>torchvision.transform.ToTensor()</code>时避免将图像的数据类型转为<code>float32</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-torchvision-transform-ToTensor-中的细节&quot;&gt;&lt;a href=&quot;#1-torchvision-transform-ToTensor-中的细节&quot; class=&quot;headerlink&quot; title=&quot;1 torchvision.transform.ToTensor()中的细节&quot;&gt;&lt;/a&gt;1 torchvision.transform.ToTensor()中的细节&lt;/h3&gt;&lt;p&gt;功能：把一个取值范围是&lt;code&gt;[0,255]&lt;/code&gt;的&lt;code&gt;PIL.Image&lt;/code&gt;或者&lt;code&gt;shape&lt;/code&gt;为&lt;code&gt;(H,W,C)&lt;/code&gt;的&lt;code&gt;numpy.ndarray&lt;/code&gt;,转换成形状为&lt;code&gt;(C,H,W)&lt;/code&gt;，取值范围是&lt;code&gt;[0,1.0]&lt;/code&gt;的&lt;code&gt;torch.FloatTensor&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;注意：只有当&lt;code&gt;numpy.ndarray&lt;/code&gt;的&lt;code&gt;dtype=unit8&lt;/code&gt;时才会将像素值scale到&lt;code&gt;[0,1.0]&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>工作一年后想说的话</title>
    <link href="http://tramac.github.io/2020/07/18/%E5%B7%A5%E4%BD%9C%E4%B8%80%E5%B9%B4%E5%90%8E%E6%83%B3%E8%AF%B4%E7%9A%84%E8%AF%9D/"/>
    <id>http://tramac.github.io/2020/07/18/%E5%B7%A5%E4%BD%9C%E4%B8%80%E5%B9%B4%E5%90%8E%E6%83%B3%E8%AF%B4%E7%9A%84%E8%AF%9D/</id>
    <published>2020-07-18T15:40:53.000Z</published>
    <updated>2020-07-20T06:15:06.486Z</updated>
    
    <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>想写点东西这个想法产生已经很久了，但是去年毕业时把博客相关的东西都搞丢了，所以博客也有一年多的时间没有更新了。正值工作一周年，内心或多或少有一些想说的话，终于下定决心重新折腾了一下，博客勉强恢复到了原来的样子，然后随便写点东西也算是对过去一年的交代。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h3&gt;&lt;p&gt;想写点东西这个想法产生已经很久了，但是去年毕业时把博客相关的东西都搞丢了，所以博客也有一年多的时间没有更新了。正值工作一周年，内心或多或少有一些想说的话，终于下定决心重新折腾了一下，博客勉强恢复到了原来的样子，然后随便写点东西也算是对过去一年的交代。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Life&amp;Thinking" scheme="http://tramac.github.io/categories/Life-Thinking/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式训练-PyTorch</title>
    <link href="http://tramac.github.io/2019/03/06/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-PyTorch/"/>
    <id>http://tramac.github.io/2019/03/06/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-PyTorch/</id>
    <published>2019-03-06T05:56:08.000Z</published>
    <updated>2020-07-18T16:04:08.092Z</updated>
    
    <content type="html"><![CDATA[<p>并行训练（数据并行与模型并行）与分布式训练是深度学习中加速训练的两种常用方式，相对于并行训练，分布式是更优的加速方案，也是PyTorch官方推荐的方法：<br>Multi-Process Single-GPU<br>This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.</p><a id="more"></a><p>个人理解：分布式训练其实也是并行训练的一种方式，只是相对于数据并行、模型并行有所不同。简单来说，分布式针对多机多卡，而数据并行针对单机多卡。</p><h3 id="Distributed-Training-Code"><a href="#Distributed-Training-Code" class="headerlink" title="Distributed Training Code"></a>Distributed Training Code</h3><p>下面内容主要指出分布式训练代码中与常规训练过程之间的主要区别。</p><h4 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h4><p>分布式训练主要涉及到的库主要有<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel" target="_blank" rel="noopener">torch.nn.parallel</a>，<a href="https://pytorch.org/docs/stable/distributed.html" target="_blank" rel="noopener">torch.distributed</a>，<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler" target="_blank" rel="noopener">torch.utils.data.distributed</a>以及<a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener">torch.multiprocessing</a>。需要注意的是我们需要把multiprocessing的start method设置为spawn或forkserver（仅Python3支持）,因为默认的方法为fork，容易导致死锁情况发生。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    torch.multiprocessing.set_start_method(<span class="string">'spawn'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.distributed</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br></pre></td></tr></table></figure><h4 id="Train-Function"><a href="#Train-Function" class="headerlink" title="Train Function"></a>Train Function</h4><p>第一个区别，在分布式训练过程中，需要设置数据的<code>non_blocking</code>的属性设置为<code>True</code>。该操作使得不同GPU上的数据副本允许重叠计算，并且可以输出训练时的统计数据，以便我们可以跟踪整个训练过程的进度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_loader, model, criterion, optimizer, epoch)</span>:</span></span><br><span class="line">    <span class="comment"># switch to train mode</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, (input, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment"># Create non_blocking tensors for distributed training</span></span><br><span class="line">        input = input.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">        target = target.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute output</span></span><br><span class="line">        output = model(input)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute gradients in a backward pass</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Call step of optimizer to update model params</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: [&#123;0&#125;][&#123;1&#125;/&#123;2&#125;]\t'</span></span><br><span class="line">                  <span class="string">'Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;)'</span>.format(</span><br><span class="line">                  epoch, i, len(train_loader), loss=losses))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span><span class="params">(initial_lr, optimizer, epoch)</span>:</span></span><br><span class="line">    <span class="string">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span></span><br><span class="line">    lr = initial_lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>))</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br></pre></td></tr></table></figure><h4 id="Validation-Function"><a href="#Validation-Function" class="headerlink" title="Validation Function"></a>Validation Function</h4><p>与训练过程相同，唯一的区别是获取数据时需要设置<code>non_blocking=True</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation</span><span class="params">(val_loader, model, criterion)</span>:</span></span><br><span class="line">    <span class="comment"># switch to evaluate mode</span></span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (input, target) <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">            input = input.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">            target = target.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># compute output</span></span><br><span class="line">            output = model(input)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'Test: [&#123;0&#125;/&#123;1&#125;]\t'</span></span><br><span class="line">                      <span class="string">'Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;))'</span>.format(</span><br><span class="line">                       i, len(val_loader), loss=losses))</span><br></pre></td></tr></table></figure><h4 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h4><p>相对于标准模型训练，分布式训练在定义数据输入时也略有不同，有些参数为分布式训练任务特定的。参数说明如下：</p><ul><li><strong>batch_size</strong>-batch size for <em>each</em> process in the distributed training group. Total batch size across distributed model is batch_size*world_size</li><li><strong>workers</strong> - number of worker processes used with the dataloaders in each process</li><li><strong>num_epochs</strong> - total number of epochs to train for</li><li><strong>starting_lr</strong> - starting learning rate for training</li><li><strong>world_size</strong> - number of processes in the distributed training environment</li><li><strong>dist_backend</strong> - backend to use for distributed training communication (i.e. NCCL, Gloo, MPI, etc.). In this tutorial, since we are using several multi-gpu nodes, NCCL is suggested.</li><li><strong>dist_url</strong> - URL to specify the initialization method of the process group. This may contain the IP address and port of the rank0 process or be a non-existant file on a shared file system. Here, since we do not have a shared file system this will incorporate the <strong>node0-privateIP</strong> and the port on node0 to use.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Collect Inputs..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch Size for training and testing</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of additional worker processes for dataloading</span></span><br><span class="line">workers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of epochs to train for</span></span><br><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting Learning Rate</span></span><br><span class="line">starting_lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of distributed processes</span></span><br><span class="line">world_size = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Distributed backend type</span></span><br><span class="line">dist_backend = <span class="string">'nccl'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Url used to setup distributed training</span></span><br><span class="line">dist_url = <span class="string">"tcp://172.31.22.234:23456"</span></span><br></pre></td></tr></table></figure><h4 id="Initialize-process-group"><a href="#Initialize-process-group" class="headerlink" title="Initialize process group"></a>Initialize process group</h4><ol><li><p>设置进程组。该过程可由函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group</span><br></pre></td></tr></table></figure><p>实现。函数的参数说明如下：</p><ul><li><strong>backend</strong>-the backend to use (i.e. NCCL, Gloo, MPI, etc.)</li><li><strong>init_method</strong>-which is either a url containing the address and port of the rank0 machine or a path to a non-existant file on the shared file system. Note, to use the file init_method, all machines must have access to the file, similarly for the url method, all machines must be able to communicate on the network so make sure to configure any firewalls and network settings to accomodate.</li><li><strong>rank</strong>-the rank of this process when run</li><li><strong>world_size</strong>-the number of processes in the collective<br>The <em>init_method</em> input can also be “env://”. In this case, the address and port of the rank0 machine will be read from the following two environment variables respectively: MASTER_ADDR, MASTER_PORT. If <em>rank*and *world_size</em> arguments are not specified in the <em>init_process_group</em> function, they both can be read from the following two environment variables respectively as well: RANK, WORLD_SIZE.</li></ul></li><li><p>设置进程的lock_rank。该操作用于为进程指定设备（即使用哪个GPU），同时也用于创建分布式数据并行模型时指定设备。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Initialize Process Group..."</span>)</span><br><span class="line"><span class="comment"># Initialize Process Group</span></span><br><span class="line"><span class="comment"># v1 - init with url</span></span><br><span class="line">dist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[<span class="number">1</span>]), world_size=world_size)</span><br><span class="line"><span class="comment"># v2 - init with file</span></span><br><span class="line"><span class="comment"># dist.init_process_group(backend="nccl", init_method="file:///home/ubuntu/pt-distributed-tutorial/trainfile", rank=int(sys.argv[1]), world_size=world_size)</span></span><br><span class="line"><span class="comment"># v3 - init with environment variables</span></span><br><span class="line"><span class="comment"># dist.init_process_group(backend="nccl", init_method="env://", rank=int(sys.argv[1]), world_size=world_size)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish Local Rank and set device on this node</span></span><br><span class="line">local_rank = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">dp_device_ids = [local_rank]</span><br><span class="line">torch.cuda.set_device(local_rank)</span><br></pre></td></tr></table></figure><h4 id="Initialize-Model"><a href="#Initialize-Model" class="headerlink" title="Initialize Model"></a>Initialize Model</h4><p>在定义模型时，需要将其指定为分布式模式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Initialize Model..."</span>)</span><br><span class="line"><span class="comment"># Construct Model</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">False</span>).cuda()</span><br><span class="line"><span class="comment"># Make model DistributedDataParallel</span></span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define loss function (criterion) and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss().cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), starting_lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure><h4 id="Initialize-Dataloaders"><a href="#Initialize-Dataloaders" class="headerlink" title="Initialize Dataloaders"></a>Initialize Dataloaders</h4><p>分布式训练的最后一个特定情况是将训练数据指定为DistributedSampler，与DistributedDataParallel模型结合使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Initialize Dataloaders..."</span>)</span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize(<span class="number">224</span>),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Datasets. STL10 will automatically download if not present</span></span><br><span class="line">trainset = datasets.STL10(root=<span class="string">'./data'</span>, split=<span class="string">'train'</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">valset = datasets.STL10(root=<span class="string">'./data'</span>, split=<span class="string">'test'</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create DistributedSampler to handle distributing the dataset across nodes when training</span></span><br><span class="line"><span class="comment"># This can only be called after torch.distributed.init_process_group is called</span></span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the Dataloaders to feed data to the training and validation steps</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>), num_workers=workers, pin_memory=<span class="literal">False</span>, sampler=train_sampler)</span><br><span class="line">val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=workers, pin_memory=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><p>开始训练，与标准模型训练唯一的不同的需要更新DistributedSampler的epoch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment"># Set epoch count for DistributedSampler</span></span><br><span class="line">    train_sampler.set_epoch(epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Adjust learning rate according to schedule</span></span><br><span class="line">    adjust_learning_rate(starting_lr, optimizer, epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># train for one epoch</span></span><br><span class="line">    print(<span class="string">"\nBegin Training Epoch &#123;&#125;"</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">    train(train_loader, model, criterion, optimizer, epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># evaluate on validation set</span></span><br><span class="line">    print(<span class="string">"Begin Validation @ Epoch &#123;&#125;"</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">    validate(val_loader, model, criterion)</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>PyTorch中，分布式训练相对于标准训练主要有以下几点不同：</p><ul><li>更改输入数据的<code>non_blocking</code>属性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = input.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">target = target.cuda(non_blocking=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li>初始化进程组，设置local rank</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[<span class="number">1</span>]), world_size=world_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Establish Local Rank and set device on this node</span></span><br><span class="line">local_rank = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">dp_device_ids = [local_rank]</span><br><span class="line">torch.cuda.set_device(local_rank)</span><br></pre></td></tr></table></figure><ul><li>指定模型为分布式数据并行</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank)</span><br></pre></td></tr></table></figure><ul><li>指定数据集为分布式样本</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><ul><li><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#writing-distributed-applications-with-pytorch" target="_blank" rel="noopener">PYTORCH 1.0 DISTRIBUTED TRAINER WITH AMAZON AWS</a></li><li><a href="https://github.com/seba-1511/dist_tuto.pth" target="_blank" rel="noopener">PyTorch Distributed Tutorial</a></li><li><a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">PyTorch ImageNet Example</a></li><li><a href="https://blog.csdn.net/zwqjoy/article/details/89415933" target="_blank" rel="noopener">Pytorch 1.0 分布式训练初探</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;并行训练（数据并行与模型并行）与分布式训练是深度学习中加速训练的两种常用方式，相对于并行训练，分布式是更优的加速方案，也是PyTorch官方推荐的方法：&lt;br&gt;Multi-Process Single-GPU&lt;br&gt;This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>并行训练-PyTorch</title>
    <link href="http://tramac.github.io/2019/03/03/%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83-PyTorch/"/>
    <id>http://tramac.github.io/2019/03/03/%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83-PyTorch/</id>
    <published>2019-03-03T08:07:18.000Z</published>
    <updated>2020-07-18T16:04:36.415Z</updated>
    
    <content type="html"><![CDATA[<p>当我们的设备具有多个GPUs时，为了训练加速，我们通常会选用多卡并行训练，常见的并行训练方式有数据并行和模型并行。而PyTorch中也给我们提供了数据并行的接口<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py" target="_blank" rel="noopener">DataParalle</a>。本文将对该并行过程做一个简单的总结。</p><a id="more"></a><h4 id="1-数据并行示意图"><a href="#1-数据并行示意图" class="headerlink" title="1. 数据并行示意图"></a>1. 数据并行示意图</h4><p><a href="https://tramac.github.io/2019/03/29/Pytorch并行训练/singleGPU.png"><img src="https://tramac.github.io/2019/03/29/Pytorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/singleGPU.png" alt="img"></a></p><p><a href="https://tramac.github.io/2019/03/29/Pytorch并行训练/parallelmodel.png"><img src="https://tramac.github.io/2019/03/29/Pytorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/parallelmodel.png" alt="img"></a></p><p><a href="https://tramac.github.io/2019/03/29/Pytorch并行训练/multiGPU.png"><img src="https://tramac.github.io/2019/03/29/Pytorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/multiGPU.png" alt="img"></a></p><p> (a) single GPU (b) parallel model (c) parallel citerion</p><h4 id="2-DataParallel过程分析"><a href="#2-DataParallel过程分析" class="headerlink" title="2. DataParallel过程分析"></a>2. DataParallel过程分析</h4><p>本文将通过分析PyTorch中<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py" target="_blank" rel="noopener">DataParallel</a>源码的方式对并行过程展开讨论。Note:本文中只展示核心代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataParallel</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, module, device_ids=None, output_device=None, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(DataParallel, self).__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *inputs, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># 1.将输入batch data分解</span></span><br><span class="line">        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line">        <span class="comment"># 2.将model在每个GPU上各复制一份</span></span><br><span class="line">        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</span><br><span class="line">        <span class="comment"># 3.进行并行前向计算</span></span><br><span class="line">        outputs = self.parallel_apply(replicas, inputs, kwargs)</span><br><span class="line">        <span class="comment"># 4.将分解后的计算结果重新聚集，合成为原始的batch大小</span></span><br><span class="line">        <span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br></pre></td></tr></table></figure><p>数据并行的主要过程就是包括上面4个步骤，结合图1(b)来看更加直观。</p><h4 id="3-DataParallelModel和DataParallelCriterion拓展"><a href="#3-DataParallelModel和DataParallelCriterion拓展" class="headerlink" title="3. DataParallelModel和DataParallelCriterion拓展"></a>3. DataParallelModel和DataParallelCriterion拓展</h4><p>在PyTorch官方提供的DataParallel接口中，是从输入x到输出y的过程，但是，在我们通常的训练过程中，我们还需要计算输入y与label之间的损失Loss，所以，为了加速到底，最好是将loss的计算也在过个GPU上进行。<a href="https://github.com/Tramac/Awesome-semantic-segmentation-pytorch/blob/master/utils/parallel.py" target="_blank" rel="noopener">DataParallelCriterion</a>的过程主要参考了<a href="https://github.com/zhanghang1989/PyTorch-Encoding" target="_blank" rel="noopener">PyTorch-Encoding</a>中的代码，代码质量很高，强烈推荐一波:+1:</p><p>在DataParallelCriterion之前，我们首先需要对原始的DataParallel进行修改，即返回结果时不让其执行gather聚集过程，因为我们接下来还要在每个分解batch上继续计算loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataParallelModel</span><span class="params">(DataParallel)</span>:</span></span><br><span class="line">    <span class="comment"># 修改gather过程，使其直接返回</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gather</span><span class="params">(self, outputs, output_device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>为了并行计算loss，主要的不同是需要将batch label同样划分为多个子batch。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataParallelCriterion</span><span class="params">(DataParallel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, *targets, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># 对label进行分解</span></span><br><span class="line">        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)</span><br><span class="line">        <span class="comment"># 将model在每个GPU上各复制一份</span></span><br><span class="line">        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</span><br><span class="line">        <span class="comment"># 执行前向计算，具体函数在这里不做展示，详细可见链接内容</span></span><br><span class="line">        outputs = criterion_parallel_apply(replicas, inputs, targets, kwargs)</span><br><span class="line">        <span class="keyword">return</span> Reduce.apply(*outputs) / len(outputs)</span><br></pre></td></tr></table></figure><p>整个并行训练过程如上所述，可结合图1(c)理解。</p><h4 id="3-并行训练中的注意事项"><a href="#3-并行训练中的注意事项" class="headerlink" title="3. 并行训练中的注意事项"></a>3. 并行训练中的注意事项</h4><ul><li>所有的划分都是在batch维度进行</li><li>batch size必须大于GPUs的数量，最好是保证是其数量的整数倍</li></ul><h4 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h4><ul><li><a href="https://github.com/Tramac/Awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Awesome-semantic-segmentation-pytorch</a></li><li><a href="https://github.com/zhanghang1989/PyTorch-Encoding" target="_blank" rel="noopener">PyTorch-Encoding</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们的设备具有多个GPUs时，为了训练加速，我们通常会选用多卡并行训练，常见的并行训练方式有数据并行和模型并行。而PyTorch中也给我们提供了数据并行的接口&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DataParalle&lt;/a&gt;。本文将对该并行过程做一个简单的总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>跨卡同步 Batch Normalization[转]</title>
    <link href="http://tramac.github.io/2019/02/25/%E8%B7%A8%E5%8D%A1%E5%90%8C%E6%AD%A5%20Batch%20Normalization[%E8%BD%AC]/"/>
    <id>http://tramac.github.io/2019/02/25/%E8%B7%A8%E5%8D%A1%E5%90%8C%E6%AD%A5%20Batch%20Normalization[%E8%BD%AC]/</id>
    <published>2019-02-25T02:33:19.000Z</published>
    <updated>2020-07-18T16:05:15.018Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://hangzhang.org/" target="_blank" rel="noopener">张航</a> Amazon AI Applied Scientist</p><p>跨卡同步（Cross-GPU Synchronized）Batch Normalization或称为同步BN（SyncBN）。</p><h4 id="写在前面：为什么要跨卡同步Batch-Normalization"><a href="#写在前面：为什么要跨卡同步Batch-Normalization" class="headerlink" title="写在前面：为什么要跨卡同步Batch Normalization"></a>写在前面：为什么要跨卡同步Batch Normalization</h4><p>现有的便准Batch Normalization因为使用数据并行（Data Parallel），是单卡的实现模式，只对单个卡上对样本进行归一化，相当于减小了批量大小（batch-size）。对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。之前在我们在图像语义分割的实验中，发现使用大规模的效果反而变差，实际上就是BN在作怪。跨卡同步Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大’了批大小，这样训练效果不再受到使用GPU数量的影响。最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验结果，所以跨卡BN已然成为竞赛刷分、发论文的必备神器。</p><a id="more"></a><h4 id="Batch-Normalization如何工作"><a href="#Batch-Normalization如何工作" class="headerlink" title="Batch Normalization如何工作"></a>Batch Normalization如何工作</h4><p>既然是技术贴，读者很多都是神学大牛，为什么还要在这里赘述BatchNorm这个简单概念呢？其实不然，很多做科研的朋友如果没有解决过相关问题，很容易混淆BN在训练和测试时候的工作方式。记得在17年CVPR的<a href="http://deeplearning.csail.mit.edu/" target="_blank" rel="noopener">tutoial</a>上，何凯明和RBG两位大神分别在自己的talk上都特意强调了BN的工作原理，可见就算台下都是CV学者，也有必要复习一遍这些知识。</p><ul><li><p>工作原理：</p><p>BN有效地加速了模型训练，加大learning rate，让模型不再过度依赖初始化。它在训练时在网络内部进行归一化（normalization），为训练提供了有效的regularization，抑制过拟合，用原作者的话时防止了协方差偏移。这里上一张图来展示训练模式的BN：</p><p><a href="https://tramac.github.io/2019/04/08/SyncBN/batchnorm.jpg"><img src="https://tramac.github.io/2019/04/08/SyncBN/batchnorm.jpg" alt="img"></a></p><p>其中输入样本，其均值为u，方差为sigma，BN的输出，时可学习对参数。个人认为，这种强大的效果其实来自于back-propagation时候，来自于均值和方差对输入样本的梯度。这也是BN在训练模式与其测试模式的重要区别，在测试模式时（evaluation mode）下，使用训练集上累积的均值和方差，在back-propagation的时候他们对输入样本没有梯度（gradient）。</p></li><li><p>数据并行：</p><p>深度学习平台在多卡（GPU）运算的时候都是采用的数据并行（DataParallel），如下图：</p><p><a href="https://tramac.github.io/2019/04/08/SyncBN/DataParallel.jpg"><img src="https://tramac.github.io/2019/04/08/SyncBN/DataParallel.jpg" alt="img"></a></p><p>每次迭代，输入被等分成多份，然后分别在不同的卡上面前向（forward）和后向（backward）运算，并且求出梯度，在迭代完成后合并梯度、更新参数，再进行下一次迭代。因为再前向和后向运算的时候，每个卡上的模型是单都运算的，所以相应的Batch Normalization也是在卡内完成，所以实际BN所归一化的样本数量仅仅局限在卡内，相当于批量大小（batch-size）见笑了。</p></li></ul><h4 id="如何实现SyncBN"><a href="#如何实现SyncBN" class="headerlink" title="如何实现SyncBN"></a>如何实现SyncBN</h4><p>跨卡同步BN的关键是前向运算的时候拿到全局的均值和方差，在后向运算时得到相应的全局梯度。最简单的实现方法是先同步求均值，再发回各卡然后同步求方差，但是这样就同步了两次。实际上需要同步一次就可以了，我们使用了一个非常简单的技巧，如下图：</p><p><a href="https://tramac.github.io/2019/04/08/SyncBN/SyncBN.jpg"><img src="https://tramac.github.io/2019/04/08/SyncBN/SyncBN.jpg" alt="img"></a></p><p>这样在前向运算的时候，我们只需要在各卡上算出与，再跨卡求出全局的和即可得到正确的均值和方差， 同理我们在后向运算的时候只需同步一次，求出相应的梯度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://hangzhang.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;张航&lt;/a&gt; Amazon AI Applied Scientist&lt;/p&gt;
&lt;p&gt;跨卡同步（Cross-GPU Synchronized）Batch Normalization或称为同步BN（SyncBN）。&lt;/p&gt;
&lt;h4 id=&quot;写在前面：为什么要跨卡同步Batch-Normalization&quot;&gt;&lt;a href=&quot;#写在前面：为什么要跨卡同步Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;写在前面：为什么要跨卡同步Batch Normalization&quot;&gt;&lt;/a&gt;写在前面：为什么要跨卡同步Batch Normalization&lt;/h4&gt;&lt;p&gt;现有的便准Batch Normalization因为使用数据并行（Data Parallel），是单卡的实现模式，只对单个卡上对样本进行归一化，相当于减小了批量大小（batch-size）。对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。之前在我们在图像语义分割的实验中，发现使用大规模的效果反而变差，实际上就是BN在作怪。跨卡同步Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大’了批大小，这样训练效果不再受到使用GPU数量的影响。最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验结果，所以跨卡BN已然成为竞赛刷分、发论文的必备神器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Finetuning with Tensorflow</title>
    <link href="http://tramac.github.io/2018/12/16/Finetuning%20with%20Tensorflow/"/>
    <id>http://tramac.github.io/2018/12/16/Finetuning%20with%20Tensorflow/</id>
    <published>2018-12-16T01:26:33.000Z</published>
    <updated>2020-07-18T15:47:27.689Z</updated>
    
    <content type="html"><![CDATA[<p>很多网络的特征提取部分都会用到fine-tunning,比如resnet-50,inception等,该文章以AlexNet为例,分析tensorflow如何进行微调</p><p><strong>finetuning的三要素:</strong></p><ul><li>预训练模型,如resnet_v2_50.npy、resnet_v2_50.ckpt等。</li><li>模型的网络结构定义。</li><li>所需从预训练模型中恢复的变量，通常以排除的方式给出。</li></ul><a id="more"></a><p><strong>Tips:</strong></p><ul><li><p>所定义网络结构中的变量名需要和预训练模型中的变量名保持相同。预训练模型中的变量名可有以下方式查看:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow</span><br><span class="line"></span><br><span class="line">reader = pywarp_tensorflow.NewCheckpointReader(checkpoint_path)</span><br><span class="line">var_to_shape_map = reader.get_variable_to_shape_map()</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> var_to_shape_map:</span><br><span class="line">    print(<span class="string">"tensor_name: "</span>, key)</span><br></pre></td></tr></table></figure></li><li><p>恢复部分变量时可借助slim。如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">variables_to_restore = slim.get_varibales_to_restore(exclude=[args.resnet_model] + <span class="string">"logits"</span>, <span class="string">"optimizer_vars"</span>,  <span class="string">"DeepLab_v3/ASPP_layer"</span>, <span class="string">"DeepLab_v3/logits"</span>])</span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">restorer.restore(sess, <span class="string">"./resnet/checkpoints/"</span> + args.resnet_model + <span class="string">"./ckpt"</span>)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很多网络的特征提取部分都会用到fine-tunning,比如resnet-50,inception等,该文章以AlexNet为例,分析tensorflow如何进行微调&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;finetuning的三要素:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预训练模型,如resnet_v2_50.npy、resnet_v2_50.ckpt等。&lt;/li&gt;
&lt;li&gt;模型的网络结构定义。&lt;/li&gt;
&lt;li&gt;所需从预训练模型中恢复的变量，通常以排除的方式给出。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Paper-reading List</title>
    <link href="http://tramac.github.io/2018/11/18/paper-reading-list/"/>
    <id>http://tramac.github.io/2018/11/18/paper-reading-list/</id>
    <published>2018-11-18T07:41:18.000Z</published>
    <updated>2021-11-16T09:31:05.756Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Visual-Representation"><a href="#Visual-Representation" class="headerlink" title="Visual Representation"></a>Visual Representation</h2><ul><li><input checked="" disabled="" type="checkbox"> Masked Autoencoders Are Scalable Vision Learners-2021&lt;<a href="https://arxiv.org/pdf/2111.06377.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Efficient Self-supervised Vision Transformers for Representation Learning-2021&lt;<a href="https://arxiv.org/pdf/2106.09785.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations-2021&lt;<a href="https://arxiv.org/abs/2106.05967" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/wvangansbeke/Revisiting-Contrastive-SSL" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li></ul><h2 id="Video-Understanding"><a href="#Video-Understanding" class="headerlink" title="Video Understanding"></a>Video Understanding</h2><ul><li><input checked="" disabled="" type="checkbox"> ViViT: A Video Vision Transformer-Arxiv2021(&lt;<a href="https://arxiv.org/pdf/2103.15691.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/rishikksh20/ViViT-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;)</li><li><input checked="" disabled="" type="checkbox"> Video Swin Transformer-Arxiv2021&lt;<a href="https://arxiv.org/pdf/2106.13230.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/SwinTransformer/Video-Swin-Transformer" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE-ICLR2021&lt;<a href="https://openreview.net/pdf?id=YicbFdNTTy" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/lucidrains/vit-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> A Survey of Transformers-Arxiv2021&lt;<a href="https://arxiv.org/pdf/2106.04554.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features-CBMI2021&lt;<a href="https://arxiv.org/pdf/2106.00358.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision-ICML2021&lt;<a href="https://arxiv.org/pdf/2102.03334.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/dandelin/ViLT" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers-CVPR2021&lt;<a href="https://arxiv.org/pdf/2103.16553.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval-2021&lt;<a href="https://arxiv.org/pdf/2104.08271.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/albanie/collaborative-experts" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Cross-Modal Retrieval Augmentation for Multi-Modal Classification-2021&lt;<a href="https://arxiv.org/pdf/2104.08108.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Continual learning in cross-modal retrieval-CVPR2021&lt;<a href="https://arxiv.org/pdf/2104.06806.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning-CVPR2021&lt;[Paper]&gt;&lt;<a href="https://github.com/researchmm/soho" target="_blank" rel="noopener">Code</a>&gt;</li><li><input disabled="" type="checkbox"> Visual Semantic Role Labeling for Video Understanding-CVPR2021&lt;<a href="https://arxiv.org/pdf/2104.00990.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/TheShadow29/VidSitu" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Perceiver: General Perception with Iterative Attention-2021&lt;<a href="https://arxiv.org/pdf/2103.03206.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning-CVPR2021&lt;<a href="https://arxiv.org/pdf/2103.13061.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/amzn/image-to-recipe-transformers" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Hyperbolic Visual Embedding Learning for Zero-Shot Recognition-CVPR2020&lt;<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ShaoTengLiu/Hyperbolic_ZSL" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Retrieve Fast, Rerank Smart:Cooperative and Joint Approaches for Improved Cross-Modal Retrieval-2021&lt;<a href="https://arxiv.org/pdf/2103.11920.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/UKPLab/MMT-Retrieval" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> What is Multimodality?&lt;<a href="https://arxiv.org/pdf/2103.06304.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Multi-modal Transformer for Video Retrieval-ECCV2020&lt;<a href="https://arxiv.org/pdf/2007.10639.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/gabeur/mmt" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Support-set Bottlenecks for Video-text Representation Learning-ICLR2021&lt;<a href="https://arxiv.org/pdf/2010.02824.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Dual Encoding for Video Retrieval by Text-TPAMI2021&lt;<a href="https://arxiv.org/pdf/2009.05381.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/danieljf24/hybrid_space" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling-CVPR2021&lt;<a href="https://arxiv.org/pdf/2102.06183.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/jayleicn/ClipBERT" target="_blank" rel="noopener">Code</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> VL-BERT: Pre-training of Generic Visual-Linguistic Representations-ICLR2020&lt;<a href="https://arxiv.org/pdf/1908.08530.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/jackroos/VL-BERT" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Transformer is All You Need:Multimodal Multitask Learning with a Unified Transformer-2021&lt;<a href="https://arxiv.org/pdf/2102.10772.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://mmf.sh/" target="_blank" rel="noopener">Code</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning-NeurIPS2020&lt;<a href="https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/gingsi/coot-videotext" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning-CVPR2020&lt;<a href="https://arxiv.org/pdf/2003.00392.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/cshizhe/hgr_v2t" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> LXMERT: Learning Cross-Modality Encoder Representations from Transformers-EMNLP2019&lt;<a href="https://arxiv.org/pdf/1908.07490.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/airsplay/lxmert" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> VisualBERT: A Simple and Performant Baseline for Vision and Language-2019&lt;<a href="https://arxiv.org/pdf/1908.03557.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/uclanlp/visualbert" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Video SemNet: Memory-Augmented Video Semantic Network-NIPS2017&lt;<a href="https://arxiv.org/pdf/2011.10909.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Self-Supervised Video Representation Learning by Pace Prediction-ECCV2020&lt;<a href="https://arxiv.org/pdf/2008.05861.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/laura-wang/video-pace" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> SalSum: Saliency-based Video Summarization using Generative Adversarial Networks-2020&lt;<a href="https://arxiv.org/pdf/2011.10432.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Self-Supervised Temporal-Discriminative Representation Learning for Video Action Recognition-2020&lt;<a href="https://arxiv.org/pdf/2008.02129.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://zhuanlan.zhihu.com/p/176774543" target="_blank" rel="noopener">Zhihu</a>&gt;</li><li><input disabled="" type="checkbox"> Classification of Important Segments in Educational Videos using Multimodal Features-CIKM2020&lt;<a href="https://arxiv.org/pdf/2010.13626.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/VideoAnalysis/EDUVSUM" target="_blank" rel="noopener">Code-Keras</a>&gt;</li><li><input disabled="" type="checkbox"> Attentive and Adversarial Learning for Video Summarization-WACV2019&lt;<a href="https://tsujuifu.github.io/pubs/wacv19_vsum-ptr-gan.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tsujuifu/pytorch_vsum-ptr-gan" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Digital Video Summarization Techniques: A Survey-2020&lt;<a href="https://www.ijert.org/digital-video-summarization-techniques-a-survey" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Emerging Trends of Multimodal Research in Vision and Language-2020&lt;<a href="https://arxiv.org/pdf/2010.09522.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> Exploring global diverse attention via pairwise temporal relation for video summarization-2020&lt;<a href="https://arxiv.org/pdf/2009.10942.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Multi-modal Dense Video Captioning-CVPR Workshops 2020&lt;<a href="https://arxiv.org/pdf/2003.07758.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/v-iashin/MDVC" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://v-iashin.github.io/mdvc" target="_blank" rel="noopener">Project</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Accuracy and Performance Comparison of Video Action Recognition Approaches-HPEC2020&lt;<a href="https://arxiv.org/pdf/2008.09037.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> What Makes Training Multi-Modal Classification Networks Hard?-CVPR2020&lt;<a href="https://arxiv.org/pdf/1905.12681.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> [<strong>DMASum</strong>] Query Twice: Dual Mixture Attention Meta Learning for Video Summarization-ACM2020&lt;<a href="https://arxiv.org/pdf/2008.08360.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input disabled="" type="checkbox"> [<strong>CHAN</strong>] Convolutional Hierarchical Attention Network for Query-Focused Video Summarization&lt;<a href="https://arxiv.org/pdf/2002.03740.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ckczzj/AAAI2020" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>ILS-SUMM</strong>] ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization-ICPR2020&lt;<a href="https://arxiv.org/pdf/1912.03650.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/YairShemer/ILS-SUMM" target="_blank" rel="noopener">Code</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward-AAAI2018&lt;<a href="https://arxiv.org/abs/1801.00054" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>SUM-GAN</strong>] Unsupervised video summarization with adversarial lstm networks-CVPR2017&lt;<a href="http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/j-min/Adversarial_Video_Summary" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Enhancing Video Summarization via Vision-Language Embedding-CVPR2017&lt;<a href="https://slazebni.cs.illinois.edu/publications/cvpr17_summarization.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Query-adaptive Video Summarization via Quality-aware Relevance Estimation-ICCV2017&lt;<a href="https://arxiv.org/pdf/1705.00581.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/arunbalajeev/query-video-summary" target="_blank" rel="noopener">Code-Theano</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Temporal Tessellation: A Unified Approach for Video Analysis-ICCV2017&lt;<a href="https://arxiv.org/pdf/1612.06950.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/dot27/temporal-tessellation" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Video Summarization with Long Short-term Memory-ECCV2016&lt;<a href="https://arxiv.org/pdf/1605.08110.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/kezhang-cs/Video-Summarization-with-LSTM" target="_blank" rel="noopener">Code-Theano</a>&gt;&lt;<a href="https://github.com/MagedMilad/Video-Summarization-with-Long-Short-term-Memory" target="_blank" rel="noopener">Code-Keras</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> Video Summarization using Deep Semantic Features-ACCV2016&lt;<a href="https://arxiv.org/pdf/1609.08758v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/mayu-ot/vsum_dsf" target="_blank" rel="noopener">Code-Chainer</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>SA-LSTM</strong>] Describing Videos by Exploiting Temporal Structure-ICCV2015&lt;<a href="https://arxiv.org/pdf/1502.08029.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yaoli/arctic-capgen-vid" target="_blank" rel="noopener">Code-Theano</a>&gt;&lt;<a href="https://github.com/hobincar/SA-LSTM" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>3D-ResNet</strong>] Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?-CVPR2018&lt;<a href="https://arxiv.org/pdf/1711.09577.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/kenshohara/3D-ResNets-PyTorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>Hidden Two-Stream</strong>] Hidden Two-Stream Convolutional Networks for Action Recognition-ACCV2018&lt;<a href="https://arxiv.org/pdf/1704.00389.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/bryanyzhu/two-stream-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> [FlowNet2] FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks&lt;[Paper(<a href="https://arxiv.org/pdf/1612.01925.pdf)]&gt;" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.01925.pdf)]&gt;</a>&lt;<a href="https://github.com/NVIDIA/flownet2-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>TSN</strong>] Temporal Segment Networks Towards Good Practices for Deep Action Recognition-ECCV2016&lt;<a href="https://arxiv.org/pdf/1608.00859.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yjxiong/temporal-segment-networks" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/yjxiong/tsn-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input disabled="" type="checkbox"> Towards Good Practices for Very Deep Two-Stream ConvNets&lt;<a href="https://arxiv.org/pdf/1507.02159.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yjxiong/caffe/tree/action_recog" target="_blank" rel="noopener">Code-Caffe</a>&gt;</li><li><input disabled="" type="checkbox"> [<strong>Two-Stream</strong>] Two-Stream Convolutional Networks for Action Recognition in Videos-NIPS2014&lt;<a href="https://arxiv.org/pdf/1406.2199.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>C3D</strong>] Learning Spatiotemporal Features with 3D Convolutional Networks-ICCV2015&lt;<a href="https://arxiv.org/pdf/1412.0767.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookarchive/C3D" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/hx173149/C3D-tensorflow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;&lt;<a href="https://github.com/jfzhang95/pytorch-video-recognition" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li><li><input checked="" disabled="" type="checkbox"> [<strong>NetVLAD</strong>] NetVLAD: CNN architecture for weakly supervised place recognition-CVPR2016&lt;<a href="https://arxiv.org/pdf/1511.07247.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Relja/netvlad" target="_blank" rel="noopener">Code-Matlab</a>&gt;&lt;<a href="https://github.com/lyakaap/NetVLAD-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li></ul><a id="more"></a><h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><p><a href="https://github.com/Tramac/Awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">GitHub</a></p><p>1.[<strong>AdaptSegNet</strong>] Learning to Adapt Structured Output Space for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.10349.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>2.[<strong>DAM/DCM</strong>] Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss-IJCAI2018&lt;<a href="https://arxiv.org/pdf/1804.10916.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>FCAN</strong>] Fully Convolutional Adaptation Networks for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1804.08286.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>4.[<strong>DenseASPP</strong>] DenseASPP for Semantic Segmentation in Street Scenes-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/DeepMotionAIResearch/DenseASPP" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>5.Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>6.[<strong>AotofocusLayer</strong>] Autofocus Layer for Semantic Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.08403.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yaq007/Autofocus-Layer" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>7.[<strong>PDV-Net</strong>] Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense V-Network-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00889-5_32.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>8.[<strong>RR-SegSE</strong>] Adaptive feature recombination and recalibration for semantic segmentation: application to brain tumor segmentation in MRI-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.02318.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sergiormpereira/rr_segse" target="_blank" rel="noopener">Code</a>&gt;<br>9.[<strong>HD-Net</strong>] Fine-Grained Segmentation Using Hierarchical Dilated Neural Networks-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_56.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>10.[<strong>U-JAPA-Net</strong>] 3D U-JAPA-Net: Mixture of Convolutional Networks for Abdominal Multi-organ CT Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_49.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>11.[<strong>CompNet</strong>] CompNet: Complementary Segmentation Network for Brain MRI Extraction-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1804.00521.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code" target="_blank" rel="noopener">Code-Keras</a>&gt;<br>12.Deep Learning-Based Boundary Detection for Model-Based Segmentation with Application to MR Prostate Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_59.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>13.[<strong>RS-Net</strong>] RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1807.10972v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/RagMeh11/RS-Net" target="_blank" rel="noopener">Code</a>&gt;<br>14.CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.04051.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>15.[<strong>CB-GANs</strong>] Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.11291.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>16.[<strong>FSENet</strong>] Focus, Segment and Erase: An Efficient Network for Multi-Label Brain Tumor Segmentation-ECCV2018&lt;<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xuan_Chen_Focus_Segment_and_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/LaviniaChen/Segment-and-Erase-Network" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>17.[<strong>DeepLabv3+</strong>] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1802.02611v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>18.[<strong>ExFuse</strong>] ExFuse: Enhancing Feature Fusion for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1804.03821.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>19.[<strong>ESPNet</strong>] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1803.06815v2.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sacmehta/ESPNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>20.[<strong>EncNet</strong>] Context Encoding for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1803.08904v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/zhanghang1989/PyTorch-Encoding" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>21.[<strong>PSPNet</strong>] Pyramid Scene Parsing Network-CVPR2017&lt;<a href="https://arxiv.org/pdf/1612.01105.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>22.[<strong>DANet</strong>] Dual Attention Network for Scene Segmentation-CVPR2019&lt;<a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/junfu1115/DANet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>23.[<strong>BiSeNet</strong>] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation-ECCV-2018&lt;<a href="https://arxiv.org/pdf/1808.00897.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ycszen/TorchSeg" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>24.[<strong>Fast-SCNN</strong>] Fast-SCNN: Fast Semantic Segmentation Network-2019&lt;<a href="https://arxiv.org/pdf/1902.04502.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/Fast-SCNN-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>25.[<strong>ICNet</strong>] ICNet for Real-Time Semantic Segmentation on High-Resolution Images-ECCV2018&lt;<a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/hszhao/ICNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>26.[<strong>DUNet</strong>] Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation-CVPR2019&lt;<a href="https://arxiv.org/abs/1903.02120.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>27.[<strong>ENet</strong>] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016&lt;<a href="https://arxiv.org/abs/1606.02147.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/TimoSaemann/ENet" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/davidtvs/PyTorch-ENet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/kwotsin/TensorFlow-ENet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>28.[<strong>CCNet</strong>] CCNet: Criss-Cross Attention for Semantic Segmentation-2018&lt;<a href="https://arxiv.org/abs/1811.11721v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/speedinghzl/CCNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>29.[<strong>OCNet</strong>] OCNet: Object Context Network for Scene Parsing-2018&lt;<a href="https://arxiv.org/abs/1809.00916.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/PkuRainBow/OCNet.pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>30.[<strong>HRNet</strong>] High-Resolution Representations for Labeling Pixels and Regions-2019&lt;<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/HRNet/HRNet-Semantic-Segmentation" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</p><h2 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h2><p>1.[<strong>Panoptic FPN</strong>] Panoptic Feature Pyramid Networks-Arxiv2019&lt;<a href="https://arxiv.org/pdf/1901.02446.pdf" target="_blank" rel="noopener">Paper</a>&gt;</p><h2 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super-Resolution"></a>Super-Resolution</h2><p>1.[<strong>mDCSRN</strong>] Efficient and Accurate MRI Super-Resolution using a Generative Adversarial Network and 3D Multi-Level Densely Connected Network-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1803.01417.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>2.[<strong>RDN</strong>] Residual Dense Network for Image Super-Resolution-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yulunzhang/RDN" target="_blank" rel="noopener">Code-Torch</a>&gt;&lt;<a href="https://github.com/thstkdgus35/EDSR-PyTorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/hengchuan/RDN-TensorFlow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;</p><h2 id="Networks-Architecture"><a href="#Networks-Architecture" class="headerlink" title="Networks Architecture"></a>Networks Architecture</h2><p>1.[<strong>DLA</strong>] Deep Layer Aggregation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/dla" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>2.[<strong>DualSkipNet</strong>] Dual Skipping Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1710.10386.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>SkipNet</strong>] SkipNet: Learning Dynamic Routing in Convolutional Networks-ECCV2018&lt;<a href="https://arxiv.org/pdf/1711.09485.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/skipnet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>4.[<strong>DRN</strong>] Dilated Residual Networks-CVPR2017&lt;<a href="https://arxiv.org/pdf/1705.09914.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/fyu/drn" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>5.[<strong>CapsNet</strong>] Dynamic Routing Between Capsules-NIPS2017&lt;<a href="https://arxiv.org/pdf/1710.09829.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/naturomics/CapsNet-Tensorflow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>6.[<strong>BlockQNN</strong>] Practical Block-wise Neural Network Architecture Generation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1708.05552.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>7.[<strong>MobileNetV2</strong>] MobileNetV2: Inverted Residuals and Linear Bottlenecks-CVPR2018&lt;<a href="https://128.84.21.199/pdf/1801.04381.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>8.[<strong>Non-Local</strong>] Non-local Neural Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1711.07971.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">Code</a>&gt;</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>1.[<strong>FocalLoss</strong>]Focal Loss for Dense Object Detection-ICCV2017&lt;<a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">Code-Caffe2</a>&gt;</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Visual-Representation&quot;&gt;&lt;a href=&quot;#Visual-Representation&quot; class=&quot;headerlink&quot; title=&quot;Visual Representation&quot;&gt;&lt;/a&gt;Visual Representation&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Masked Autoencoders Are Scalable Vision Learners-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2111.06377.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Efficient Self-supervised Vision Transformers for Representation Learning-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2106.09785.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/abs/2106.05967&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/wvangansbeke/Revisiting-Contrastive-SSL&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Video-Understanding&quot;&gt;&lt;a href=&quot;#Video-Understanding&quot; class=&quot;headerlink&quot; title=&quot;Video Understanding&quot;&gt;&lt;/a&gt;Video Understanding&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; ViViT: A Video Vision Transformer-Arxiv2021(&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.15691.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/rishikksh20/ViViT-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Video Swin Transformer-Arxiv2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2106.13230.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/SwinTransformer/Video-Swin-Transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE-ICLR2021&amp;lt;&lt;a href=&quot;https://openreview.net/pdf?id=YicbFdNTTy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/lucidrains/vit-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; A Survey of Transformers-Arxiv2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2106.04554.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features-CBMI2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2106.00358.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision-ICML2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2102.03334.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/dandelin/ViLT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers-CVPR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.16553.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2104.08271.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/albanie/collaborative-experts&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Cross-Modal Retrieval Augmentation for Multi-Modal Classification-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2104.08108.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Continual learning in cross-modal retrieval-CVPR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2104.06806.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning-CVPR2021&amp;lt;[Paper]&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/researchmm/soho&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Visual Semantic Role Labeling for Video Understanding-CVPR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2104.00990.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/TheShadow29/VidSitu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Perceiver: General Perception with Iterative Attention-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.03206.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning-CVPR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.13061.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/amzn/image-to-recipe-transformers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Hyperbolic Visual Embedding Learning for Zero-Shot Recognition-CVPR2020&amp;lt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/ShaoTengLiu/Hyperbolic_ZSL&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Retrieve Fast, Rerank Smart:Cooperative and Joint Approaches for Improved Cross-Modal Retrieval-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.11920.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/UKPLab/MMT-Retrieval&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; What is Multimodality?&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2103.06304.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Multi-modal Transformer for Video Retrieval-ECCV2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2007.10639.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/gabeur/mmt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Support-set Bottlenecks for Video-text Representation Learning-ICLR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2010.02824.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Dual Encoding for Video Retrieval by Text-TPAMI2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2009.05381.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/danieljf24/hybrid_space&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling-CVPR2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2102.06183.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/jayleicn/ClipBERT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; VL-BERT: Pre-training of Generic Visual-Linguistic Representations-ICLR2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1908.08530.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/jackroos/VL-BERT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Transformer is All You Need:Multimodal Multitask Learning with a Unified Transformer-2021&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2102.10772.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://mmf.sh/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning-NeurIPS2020&amp;lt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/gingsi/coot-videotext&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning-CVPR2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2003.00392.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/cshizhe/hgr_v2t&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; LXMERT: Learning Cross-Modality Encoder Representations from Transformers-EMNLP2019&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1908.07490.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/airsplay/lxmert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; VisualBERT: A Simple and Performant Baseline for Vision and Language-2019&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1908.03557.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/uclanlp/visualbert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Video SemNet: Memory-Augmented Video Semantic Network-NIPS2017&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2011.10909.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Self-Supervised Video Representation Learning by Pace Prediction-ECCV2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2008.05861.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/laura-wang/video-pace&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; SalSum: Saliency-based Video Summarization using Generative Adversarial Networks-2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2011.10432.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Self-Supervised Temporal-Discriminative Representation Learning for Video Action Recognition-2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2008.02129.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/176774543&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Zhihu&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Classification of Important Segments in Educational Videos using Multimodal Features-CIKM2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2010.13626.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/VideoAnalysis/EDUVSUM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Keras&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Attentive and Adversarial Learning for Video Summarization-WACV2019&amp;lt;&lt;a href=&quot;https://tsujuifu.github.io/pubs/wacv19_vsum-ptr-gan.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/tsujuifu/pytorch_vsum-ptr-gan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Digital Video Summarization Techniques: A Survey-2020&amp;lt;&lt;a href=&quot;https://www.ijert.org/digital-video-summarization-techniques-a-survey&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Emerging Trends of Multimodal Research in Vision and Language-2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2010.09522.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Exploring global diverse attention via pairwise temporal relation for video summarization-2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2009.10942.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Multi-modal Dense Video Captioning-CVPR Workshops 2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2003.07758.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/v-iashin/MDVC&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://v-iashin.github.io/mdvc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Project&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Accuracy and Performance Comparison of Video Action Recognition Approaches-HPEC2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2008.09037.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; What Makes Training Multi-Modal Classification Networks Hard?-CVPR2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1905.12681.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;DMASum&lt;/strong&gt;] Query Twice: Dual Mixture Attention Meta Learning for Video Summarization-ACM2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2008.08360.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;CHAN&lt;/strong&gt;] Convolutional Hierarchical Attention Network for Query-Focused Video Summarization&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/2002.03740.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/ckczzj/AAAI2020&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;ILS-SUMM&lt;/strong&gt;] ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization-ICPR2020&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1912.03650.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/YairShemer/ILS-SUMM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward-AAAI2018&amp;lt;&lt;a href=&quot;https://arxiv.org/abs/1801.00054&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/KaiyangZhou/pytorch-vsumm-reinforce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;SUM-GAN&lt;/strong&gt;] Unsupervised video summarization with adversarial lstm networks-CVPR2017&amp;lt;&lt;a href=&quot;http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/j-min/Adversarial_Video_Summary&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Enhancing Video Summarization via Vision-Language Embedding-CVPR2017&amp;lt;&lt;a href=&quot;https://slazebni.cs.illinois.edu/publications/cvpr17_summarization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Query-adaptive Video Summarization via Quality-aware Relevance Estimation-ICCV2017&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1705.00581.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/arunbalajeev/query-video-summary&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Theano&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Temporal Tessellation: A Unified Approach for Video Analysis-ICCV2017&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1612.06950.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/dot27/temporal-tessellation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Tensorflow&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Video Summarization with Long Short-term Memory-ECCV2016&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1605.08110.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/kezhang-cs/Video-Summarization-with-LSTM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Theano&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/MagedMilad/Video-Summarization-with-Long-Short-term-Memory&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Keras&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Video Summarization using Deep Semantic Features-ACCV2016&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1609.08758v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/mayu-ot/vsum_dsf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Chainer&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;SA-LSTM&lt;/strong&gt;] Describing Videos by Exploiting Temporal Structure-ICCV2015&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1502.08029.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/yaoli/arctic-capgen-vid&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Theano&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/hobincar/SA-LSTM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;3D-ResNet&lt;/strong&gt;] Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?-CVPR2018&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1711.09577.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/kenshohara/3D-ResNets-PyTorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;Hidden Two-Stream&lt;/strong&gt;] Hidden Two-Stream Convolutional Networks for Action Recognition-ACCV2018&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1704.00389.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/bryanyzhu/two-stream-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [FlowNet2] FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks&amp;lt;[Paper(&lt;a href=&quot;https://arxiv.org/pdf/1612.01925.pdf)]&amp;gt;&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1612.01925.pdf)]&amp;gt;&lt;/a&gt;&amp;lt;&lt;a href=&quot;https://github.com/NVIDIA/flownet2-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;TSN&lt;/strong&gt;] Temporal Segment Networks Towards Good Practices for Deep Action Recognition-ECCV2016&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1608.00859.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/yjxiong/temporal-segment-networks&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Caffe&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/yjxiong/tsn-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; Towards Good Practices for Very Deep Two-Stream ConvNets&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1507.02159.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/yjxiong/caffe/tree/action_recog&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Caffe&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;Two-Stream&lt;/strong&gt;] Two-Stream Convolutional Networks for Action Recognition in Videos-NIPS2014&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1406.2199.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;C3D&lt;/strong&gt;] Learning Spatiotemporal Features with 3D Convolutional Networks-ICCV2015&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1412.0767.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/facebookarchive/C3D&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Caffe&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/hx173149/C3D-tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Tensorflow&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/jfzhang95/pytorch-video-recognition&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; [&lt;strong&gt;NetVLAD&lt;/strong&gt;] NetVLAD: CNN architecture for weakly supervised place recognition-CVPR2016&amp;lt;&lt;a href=&quot;https://arxiv.org/pdf/1511.07247.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/Relja/netvlad&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-Matlab&lt;/a&gt;&amp;gt;&amp;lt;&lt;a href=&quot;https://github.com/lyakaap/NetVLAD-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code-PyTorch&lt;/a&gt;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper-reading List" scheme="http://tramac.github.io/categories/Paper-reading-List/"/>
    
    
      <category term="Paper" scheme="http://tramac.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch参数初始化及Finetunning</title>
    <link href="http://tramac.github.io/2018/11/06/PyTorch%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8AFinetunning/"/>
    <id>http://tramac.github.io/2018/11/06/PyTorch%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8AFinetunning/</id>
    <published>2018-11-06T06:25:53.000Z</published>
    <updated>2020-07-18T16:05:38.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>PyTorch的代码中很多都有显式的参数初始化过程（默认的初始化形式是什么？），本文将对其做一个总结。</p><a id="more"></a><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>参数的初始化其实就是对参数进行赋值。网络中所需要学习的参数其实都是<code>Variable</code>，它其实是对<code>Tensor</code>的封装，同时提供了<code>data</code>，<code>grad</code>等接口，这就意味着我们可以直接对这些参数进行赋值操作。这就是PyTorch简洁高效所在。</p><p><a href="https://tramac.github.io/2019/04/08/PyTorch参数初始化及Finetunning/variable.png"><img src="https://tramac.github.io/2019/04/08/PyTorch%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8AFinetunning/variable.png" alt="img"></a></p><p>PyTorch中有多种初始化方式，但是其作者所推崇的方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">        n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">        m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">    <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">        m.bias.data.zero_()</span><br></pre></td></tr></table></figure><h2 id="Finetunning"><a href="#Finetunning" class="headerlink" title="Finetunning"></a>Finetunning</h2><p>通常在加载了预训练模型的参数之后，我们需要finetune模型，一般有以下两种方式：</p><h4 id="局部微调"><a href="#局部微调" class="headerlink" title="局部微调"></a>局部微调</h4><p>冻结其它层，只调节最后的几层。其实不训练也就意味着不进行梯度计算，PyTorch中提供的<code>requires_grad</code>使得对训练的控制变得非常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 替换最后的全连接层，新构造的模块的参数默认requires_grad=True</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只优化最后的分类层</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="全局微调"><a href="#全局微调" class="headerlink" title="全局微调"></a>全局微调</h4><p>对全局都进行finetune，但是希望该换过的层和其它层的学习率不同，这是可以把其它层和新层在<code>optimizer</code>中单独赋予不同的学习率。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ignored_params = list(map(id, model.fc.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params,</span><br><span class="line">                     model.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([</span><br><span class="line">            &#123;<span class="string">'params'</span>: base_params&#125;,</span><br><span class="line">            &#123;<span class="string">'params'</span>: model.fc.parameters(), <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>其中<code>base_params</code>使用<code>1e-3</code>来训练，<code>model.fc.parameters</code>使用<code>1e-2</code>来训练，<code>momentum</code>是二者共有的。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.csdn.net/u012759136/article/details/65634477" target="_blank" rel="noopener">PyTorch参数初始化和Finetune</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;PyTorch的代码中很多都有显式的参数初始化过程（默认的初始化形式是什么？），本文将对其做一个总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://tramac.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>第9章 图像局部与分割</title>
    <link href="http://tramac.github.io/2018/09/22/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC9%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8E%E5%88%86%E5%89%B2/"/>
    <id>http://tramac.github.io/2018/09/22/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC9%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8E%E5%88%86%E5%89%B2/</id>
    <published>2018-09-22T11:09:23.000Z</published>
    <updated>2020-07-18T16:08:24.868Z</updated>
    
    <content type="html"><![CDATA[<h4 id="局部与分割"><a href="#局部与分割" class="headerlink" title="局部与分割"></a>局部与分割</h4><p>简单来讲，分割就是如何从图像中将目标或部分目标分割出来。处了从图像中分割出前景目标之外，在很多情况下我们也希望将感兴趣的目标区域分割出来。<br>本章主要研究其他用于查找、填充和分离一幅图像中的目标以及部分目标物体的算法。</p><a id="more"></a><h4 id="背景减除（背景差分）"><a href="#背景减除（背景差分）" class="headerlink" title="背景减除（背景差分）"></a>背景减除（背景差分）</h4><p>由于背景减除简单而且摄像机在很多情况下是固定的，在视频安全领域，背景减除也许是最基本的图像处理操作。所以，背景模型的建立至关重要。<br>一旦背景模型建立，将背景模型和当前的图像进行比较，然后减去这些已知的背景减除，则剩下的目标物大致就是所求的前景目标了。<br>当然，“背景”在不同的应用场合下是一个很难定义的问题，后续会介绍一些背景建模的方法。</p><ul><li><strong>背景减除的缺点</strong></li></ul><p>背景减除的一个缺点是建立在一个不常成立的假设：所有像素点是独立的。因为这种建模方法在计算像素变化时并没有考虑它相邻的像素。<br>其中一种解决方式是建立一个多元模型，它把基本的像素独立模型扩展为包含了相邻像素的亮度的基本场景。在这种情况下，我们用相邻像素的亮度来区别相邻像素值的相对明暗。但是这种模型消耗两倍的内存和更多的计算量。<br>但是在实际应用中，由于额外的开销，通常会避免使用复杂的模型。当像素独立假设不成立情况下，我们可以更有效地把精力投入到清除那些错误的检测结果中。本章将通过使用连通域，并且后续的方法严格限制在像素变化独立的假设基础上。</p><ul><li><strong>场景建模</strong></li></ul><p>通常，一个场景模型可能包含许多层次，从新的场景到旧的场景再到背景。</p><ul><li><strong>像素片段</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对任意直线上的像素进行采样</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvInitLineIterator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* <span class="built_in">image</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint pt1,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint pt2,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvLineIterator* line_iterator,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> connectivity=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> left_to_right=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>帧差</strong></li></ul><p>最简单的背景减除方法就是用一帧减去另一帧，然后将足够大的差别标为前景。这种方法往往能捕捉运动目标的边缘。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算帧差</span></span><br><span class="line">cvAbsDiff(frameTime1, frameTime2, frameForeground);</span><br></pre></td></tr></table></figure><p>由于像素值总会受到噪声和波动的影响，我们应该忽略很小的差异，标识其余的作为较大的差别：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cvThreshold(frameForeground, frameForeground, <span class="number">15</span>, <span class="number">255</span>, CV_THRESH_BINARY);</span><br></pre></td></tr></table></figure><ul><li><strong>平均背景法</strong></li></ul><p>平均背景法的基本思路是计算每个像素的平均值和标准差作为它的背景模型。</p><ul><li><strong>累积均值、方差和协方差</strong></li></ul><p>在平均背景法中，一个常用的函数是累积函数cvAcc()。通过该操作，我们可以计算出整个场景或部分场景的基本统计特性（均值，方差和协方差）。<br><strong>均值漂移值</strong>：通过大量图像计算每个像素的均值的最简单的方法就是调用函数cvAcc()把他们加起来再除以图像总数来获得均值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvAcc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* <span class="built_in">image</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* sum,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mask=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 直接计算均值漂移</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvRunningAvg</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* <span class="built_in">image</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* acc,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> alpha,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mask=<span class="literal">NULL</span>;</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><p><strong>计算方差</strong>：可以通过累积平方图像，快速计算单个像素的方差。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSquareAcc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* <span class="built_in">image</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* sqsum,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mask=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>计算协方差</strong>：可以通过选择一个特定的时间间隔来观测图像是怎么变化的，然后用当前图像乘以和特定时间间隔相对应的图像。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算协方差</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMultiplyAcc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* image1,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* image2,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* acc,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mask=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>高级背景模型</strong></li></ul><p>codebook由一些boxes组成，这些boxes包含很长时间不变的像素值。codebook方法能够解决像素剧烈变化的问题。<br><strong>结构</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// codebook结构体定义</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">code_book</span>&#123;</span></span><br><span class="line">    code_element **cb;</span><br><span class="line">    <span class="keyword">int</span> numEntries;</span><br><span class="line">    <span class="keyword">int</span> t;</span><br><span class="line">&#125; codebook;</span><br></pre></td></tr></table></figure><p><strong>背景学习</strong><br>略<br><strong>学习有移动前景目标的背景</strong><br>略<br><strong>背景差分：寻找前景目标</strong><br>略<br><strong>使用codebook背景模型流程总结</strong>:<br>1.使用函数update_codebook()在几秒钟或几分钟时间内训练一个基本的背景模型。<br>2.调用函数clear_stale_entries()清除stale索引。<br>3.调整阈值minMod和maxMod对已知前景达到最好的分割。<br>4.保持一个更高级别的场景模型。<br>5.通过函数background_diff()使用训练好的模型将前景从背景中分割出来。<br>6.定期更新学习的背景像素。<br>7.在一个频率较慢的情况下，用函数clear_stale_entries()定期清理stale的codebook索引。</p><ul><li><strong>用于前景清除的连通部分</strong></li></ul><p>使用连通成分分析来清理原始分割图像是另一种寻找轮廓的方法。它能从原始噪声掩模图像创建出完整的掩模图像。连通域法是一个功能强大的在背景减去中去除噪声的技术。<br>下面直接给出一些处理连通区域的主要功能：<br>1.采用多边形拟合存在的轮廓部分，或凸包设置连通轮廓有多大<br>2.设置连通轮廓的大小以保证不被删除<br>3.设置返回的连通轮廓的最大数目<br>4.可选返回存活的连通轮廓的外接矩形<br>5.可选返回存活连通轮廓的中心</p><h4 id="分水岭算法"><a href="#分水岭算法" class="headerlink" title="分水岭算法"></a>分水岭算法</h4><p>在许多实际情况下，我们要分割图像，但无法从背景图像中获得有用信息。分水岭算法在这方面往往是有效的。<br>更确切的说，分水岭算法允许用户来标记目标的某个部分为目标，或背景的某个部分为背景。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分水岭算法函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvWatershed</span><span class="params">(<span class="keyword">const</span> CvArr* <span class="built_in">image</span>, CvArr* markers)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="用Inpainting修补图像"><a href="#用Inpainting修补图像" class="headerlink" title="用Inpainting修补图像"></a>用Inpainting修补图像</h4><p>图像常常被噪声腐蚀。这些噪声也许是镜头上的灰尘或水滴造成的，也可能是旧照片上的划痕，或者图像的部分已经被破坏了。Inpainting是修复这些损害的一种有效方法，它可以利用这些已经破坏区域的边缘的颜色和结构，繁殖和混合到损坏的图像里面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvInpaint</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* src,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mask,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* dst,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> inpaintRadius,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> flags</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="均值漂移分割"><a href="#均值漂移分割" class="headerlink" title="均值漂移分割"></a>均值漂移分割</h4><p>均值漂移能沿时间轴找到颜色空间的峰值分布。这里均值漂移分割能找到在空间上颜色分布的峰值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 均值漂移分割</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPyrMeanShiftFiltering</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* src,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* dst,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> spatialRadius,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> colorRadius,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> max_level=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvTermCriteria termcrit=cvTermCriteria(</span></span></span><br><span class="line"><span class="function"><span class="params">        CV_TERMCRIT_ITER | CV_TERMCRIT_EPS,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="number">1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    )</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;局部与分割&quot;&gt;&lt;a href=&quot;#局部与分割&quot; class=&quot;headerlink&quot; title=&quot;局部与分割&quot;&gt;&lt;/a&gt;局部与分割&lt;/h4&gt;&lt;p&gt;简单来讲，分割就是如何从图像中将目标或部分目标分割出来。处了从图像中分割出前景目标之外，在很多情况下我们也希望将感兴趣的目标区域分割出来。&lt;br&gt;本章主要研究其他用于查找、填充和分离一幅图像中的目标以及部分目标物体的算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第8章 轮廓</title>
    <link href="http://tramac.github.io/2018/09/18/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC8%E7%AB%A0%20%E8%BD%AE%E5%BB%93/"/>
    <id>http://tramac.github.io/2018/09/18/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC8%E7%AB%A0%20%E8%BD%AE%E5%BB%93/</id>
    <published>2018-09-18T11:55:08.000Z</published>
    <updated>2020-07-18T16:08:13.888Z</updated>
    
    <content type="html"><![CDATA[<h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><p>OpenCV使用内存存储器(memory storage)来统一管理各种<strong>动态</strong>对象的内存。内存存储器在底层被实现为一个有许多相同大小的内存块组成的双向链表，通过这种结构，OpenCV可以从内存存储器中快速的分配内存或将内存返回给内存存储器。</p><a id="more"></a><p>内存存储器可以通过以下四个函数访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个内存存储器</span></span><br><span class="line"><span class="function">CvMemStorage* <span class="title">cvCreateMemStorage</span><span class="params">(<span class="keyword">int</span> block_size=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="comment">// 释放内存存储器中分配的内存</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvReleaseMemStorage</span><span class="params">(CvMemStorage** storage)</span></span>;</span><br><span class="line"><span class="comment">// 清空内存存储器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvClearMemStorage</span><span class="params">(CvMemStorage* storage)</span></span>;</span><br><span class="line"><span class="comment">// 从一个内存存储器中申请空间</span></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">cvMemStorageAlloc</span><span class="params">(CvMemStorage* storage, <span class="keyword">size_t</span> <span class="built_in">size</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h4><p>序列是内存存储器中可以存储的一种对象。序列是某种结构的链表。OpenCV中序列可以存储多种不同的结构。序列在内存被实现为一个双端队列(deque)。因此，序列可以实现快速的随机访问，以及快速删除顶端的元素，但是从中间删除元素则稍慢些。<br>结构CVSeq的定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CvSeq</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> flags;</span><br><span class="line">    <span class="keyword">int</span> header_size;</span><br><span class="line">    CvSeq* h_prev;<span class="comment">// 指向前一序列</span></span><br><span class="line">    CvSeq* h_next;<span class="comment">// 指向下一序列</span></span><br><span class="line">    CvSeq* v_prev;</span><br><span class="line">    CvSeq* v_next;</span><br><span class="line">    <span class="keyword">int</span> total;</span><br><span class="line">    <span class="keyword">int</span> elem_size;</span><br><span class="line">    <span class="keyword">char</span>* block_max;</span><br><span class="line">    <span class="keyword">char</span>* ptr;</span><br><span class="line">    <span class="keyword">int</span> delta_elems;</span><br><span class="line">    CvMemStorage* storage;</span><br><span class="line">    CvSeqBlock* free_blocks;</span><br><span class="line">    CvSeqBlock* first;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>创建序列</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 序列分配函数</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvCreateSeq</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> seq_flags,<span class="comment">// 组织数据的方式</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> header_size,<span class="comment">// 序列的头大小</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> elem_size,<span class="comment">// 序列要存储的元素的大小</span></span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage<span class="comment">// 指定内存存储器</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>删除序列</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 清空序列中的所有元素</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvClearSeq</span><span class="params">(CvSeq* seq)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>直接访问序列中的元素</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 随机访问某个元素</span></span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cvGetSeqElem</span><span class="params">(seq, index)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印序列中的所有碘元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; seq-&gt;total; ++i) &#123;</span><br><span class="line">    CvPoint* p = (CvPoint*)cvGetSeqElem(seq, i);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"(%d, %d)\n"</span>, p-&gt;x, p-&gt;y);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 检测一个元素是否在序列中</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvSeqElemIdx</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span>* element,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeqBlock** block=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>切片、复制和移动序列中的数据</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 深度复制一个序列，并创建一个完全独立的序列结构</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvCloneSeq</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 复制一个序列的子序列，或者仅为子序列创建一个头，和原来序列共用元素空间</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvSeqSlice</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSlice slice,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage=<span class="literal">NULL</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> copy_data=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 删除序列</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqRemoveSlice</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSlice slice</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 添加序列</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqInsertSlice</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> before_index,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* from_arr</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 通过比较函数，对序列中的元素进行排序，或搜索一个序列</span></span><br><span class="line"><span class="comment">// 函数原型</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*CvCmpFunc)</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span>* a, <span class="keyword">const</span> <span class="keyword">void</span>* b, <span class="keyword">void</span>* userdata)</span></span>;</span><br><span class="line"><span class="comment">// 对序列排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqSort</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvCmpFunc func,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* userdata=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cvSeqSearch</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span>* elem,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvCmpFunc func,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> is_sorted,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>* elem_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* userdata=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 对序列进行逆序操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqInvert</span><span class="params">(CvSeq* seq)</span></span>;</span><br><span class="line"><span class="comment">// 拆分序列</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvSeqPartition</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq** labels,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvCmpFunc is_equal,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* userdata</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>将序列作为栈来使用</strong></li></ul><p>序列内部实现是一个双端队列。因此，我们可以高效地从序列的任意一段访问序列，这样我们可以将学列当做一个栈使用。<br>下面给出一些关于栈使用的函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cvSeqPush</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* element=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cvSeqPushFront</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* element=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqPop</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* element=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqPopFront</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* element=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqPushMulti</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* elements, <span class="keyword">int</span> count, <span class="keyword">int</span> in_front=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqPopMulti</span><span class="params">(CvSeq* seq, <span class="keyword">void</span>* elements, <span class="keyword">int</span> count, <span class="keyword">int</span> in_front=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>插入和删除元素</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">char* cvSeqInsert(CvSeq* seq, int before_index), void* element=NULL;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSeqRemove</span><span class="params">(CvSeq* seq, <span class="keyword">int</span> index)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>序列中块的大小</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 改变块的大小</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetSeqBlockSize</span><span class="params">(CvSeq* seq, <span class="keyword">int</span> delta_elems)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>序列的读取和写入</strong></li></ul><p>当使用序列时，如果需要最高的性能，可以使用一些特殊的函数修改序列。这些函数通过专门的结构来保存序列的当前状态，这样使得很多后续操作都可以在更短的时间内完成。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化序列写结构CvSeqWriter</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvStartWriteSeq</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> seq_flags,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> header_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> elem_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeqWriter* writer</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 初始化写状态结构到序列的末尾</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvStartAppendToSeq</span><span class="params">(CvSeq* seq, CvSeqWriter* writer)</span></span>;</span><br><span class="line"><span class="comment">// 关闭写状态，使得写入生效</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvEndWriteSeq</span><span class="params">(CvSeqWriter* writer)</span></span>;</span><br><span class="line"><span class="comment">// 刷新写操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvFlushSeqWriter</span><span class="params">(CvSeqWriter* writer)</span></span>;</span><br><span class="line"><span class="comment">// 写入元素</span></span><br><span class="line">CV_WRITE_SEQ_ELEM(elem, writer);</span><br><span class="line">CV_WRITE_SEQ_ELEM_VAR(elem_ptr, writer);</span><br></pre></td></tr></table></figure><p>与之对应的读操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化读结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvStartReadSeq</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeqReader* reader,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> reverse=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 返回读状态在序列中当前的位置</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvGetSeqReaderPos</span><span class="params">(CvSeqReader* reader)</span></span>;</span><br><span class="line"><span class="comment">// 设置读操作的新位置</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetSeqReaderPos</span><span class="params">(CvSeqReader* reader, <span class="keyword">int</span> index, <span class="keyword">int</span> is_relative=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="comment">// 读取元素</span></span><br><span class="line">CV_NEXT_SEQ_ELEM(elem_size, reader);</span><br><span class="line">CV_PREV_SEQ_ELEM(elem_size, reader);</span><br><span class="line">CV_READ_SEQ_ELEM(elem, reader);</span><br><span class="line">CV_REV_READ_SEQ_ELEM(elem, reader);</span><br></pre></td></tr></table></figure><ul><li><strong>序列和数组</strong></li></ul><p>有时候可能需要将序列转换成数组。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 复制序列的全部或部分到一个连续内存数组中</span></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">cvCvtSeqToArray</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* elements,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSlice slice=CV_WHOLE_SEQ</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 将数组转换为序列</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvMakeSeqHeaderForArray</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> seq_type,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> header_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> elem_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* elements,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> total,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeqBlock* block</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="查找轮廓"><a href="#查找轮廓" class="headerlink" title="查找轮廓"></a>查找轮廓</h4><p>一个轮廓一般对应一系列的点，也就是图像中的一条曲线。表示方法可能根据不同情况而有所不同。有多种方法可以表示曲线。在OpenCV中一般用序列来存储轮廓信息。序列中的每一个元素是曲线中一个点的位置。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 寻找轮廓函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvFindContours</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    IplImage* img,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq** firstContour,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> headerSize=<span class="keyword">sizeof</span>(CvContour),</span></span></span><br><span class="line"><span class="function"><span class="params">    CvContourRetrievalMode mode=CV_RETR_LIST,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvChainApproxMethod method=CV_CHAIN_APPROX_SIMPLE</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>使用序列表示轮廓</strong></li></ul><p>序列中保存一系列的点，这些点构成轮廓。轮廓是点的序列，可以用来表示图像空间中的曲线。<br>常用的处理函数:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 与CVFindContours()类似，不过每次返回一个轮廓</span></span><br><span class="line"><span class="function">CvContourScanner <span class="title">cvStartFindContours</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* <span class="built_in">image</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> header_size=<span class="keyword">sizeof</span>(CvContour),</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> mode=CV_RETR_LIST,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method=CV_CHAIN_APPROX_SIMPLE,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint offset=cvPoint(<span class="number">0</span>,<span class="number">0</span>)</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 查找下一个轮廓</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvFindNextContour</span><span class="params">(CvContourScanner scanner)</span></span>;</span><br><span class="line"><span class="comment">// 轮廓替换</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSubstituteContour</span><span class="params">(CvContourScanner scanner, CvSeq* new_counter)</span></span>;</span><br><span class="line"><span class="comment">// 结束查找</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvEndFindContour</span><span class="params">(CvContourScanner* scanner)</span></span>;</span><br><span class="line"><span class="comment">// 将Freeman链转换为多边形表示</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvApproxChains</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* src_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method=CV_CHAIN_APPROX_SIMPLE,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> minimal_perimeter=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> recursive=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>Freeman链码</strong></li></ul><p>一般情况下，通过cvFindContours获取的轮廓是一系列顶点的序列。另一种表达方法是当mehod设置为CV_CHAIN_CODE时，生成的轮廓是通过Freeman链码的方式返回。在Freeman链码中，多边形被表示为一系列的位移，每一个位移有8个方向，使用0-7表示。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化Freeman链cvChainPtReader结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvStartReadChainPoints</span><span class="params">(CvChain* chain, CvChainPtReader* reader)</span></span>;</span><br><span class="line"><span class="comment">// 读取链码中的每个点</span></span><br><span class="line"><span class="function">CvPoint <span class="title">cvReadChainPoint</span><span class="params">(CvChainPtReader* reader)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>绘制轮廓</strong></li></ul><p>一个经常用到的功能是在屏幕上绘制检测到的轮廓。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDrawContours</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvArr* img,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* contour,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvScalar external_color,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvScalar holo_color,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> max_level,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> thickness=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> line_type=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint offset=cvPoint(<span class="number">0</span>, <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="深入分析轮廓"><a href="#深入分析轮廓" class="headerlink" title="深入分析轮廓"></a>深入分析轮廓</h4><p>当分析一个图像的时候，针对轮廓常用的操作有识别和处理，另外相关的还有多种对轮廓的处理，如简化或拟合轮廓，匹配轮廓到模板。</p><ul><li><strong>多边形逼近</strong></li></ul><p>当我们绘制一个多边形或者进行形状分析的时候，通常需要使用多边形逼近一个轮廓，使得定点数目变少。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 轮廓逼近函数</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvApproxPoly</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span>* src_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> header_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> recursive=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 寻找关键点函数</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvFindDominantPoints</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeq* contour,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method=CV_DOMINANT_IPAN,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter1=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter2=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter3=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter4=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><ul><li><strong>特性概括</strong></li></ul><p>轮廓处理中经常遇到的另一个任务是计算一些轮廓变化的概括特性。这可能包含长度或其他一些反映轮廓整体大小的度量。另一个有用的特性是轮廓矩，可以用来概括轮廓的总形状特性。<br><strong>长度</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算轮廓的长度</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvArcLength</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span>* curve, CvSlice slice=CV_WHOLE_SEQ, <span class="keyword">int</span> is_closed=<span class="number">-1</span>)</span></span>;</span><br><span class="line"><span class="comment">// 计算轮廓的面积</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvContourArea</span><span class="params">(<span class="keyword">const</span> CvArr* contour, CvSlice slice=CV_WHOLE_SEQ)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>边界框</strong><br>长度和面积只是轮廓的简单特性，更复杂一些的特性描述应该是矩形边界框，圆形边界框或椭圆边界框。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取矩形边界框</span></span><br><span class="line"><span class="function">CvRect <span class="title">cvBoundingRect</span><span class="params">(CvArr* points, <span class="keyword">int</span> update=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="comment">// 获取一个包围轮廓最小的长方形</span></span><br><span class="line"><span class="function">CvBox2D <span class="title">cvMInAreaRect2</span><span class="params">(<span class="keyword">const</span> CvArr* points, cvMemStorage* storage=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="comment">// CvBox2D结构体的定义</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> struct <span class="title">CvBox2D</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint2D32f center;</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSize2D32f <span class="built_in">size</span>;</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">float</span> angle;</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> cvBox2D</span>;</span><br></pre></td></tr></table></figure><p><strong>圆形和椭圆形边界</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取圆形外接圆</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvMInEnclosingCircle</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* points,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint2D32f* center,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">float</span>* radius</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 获取最佳拟合椭圆</span></span><br><span class="line"><span class="function">CvBox2D <span class="title">cvFitEllipse2</span><span class="params">(<span class="keyword">const</span> CvArr* points)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>几何</strong></li></ul><p>在处理CVBox2D或多边形边界的时候，经常需要进行多边形以及边界框的重叠判断。下面是一些常用的函数:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据输入的2个矩形计算，得出最小外包矩形</span></span><br><span class="line"><span class="function">CvRect <span class="title">cvMaxRect</span><span class="params">(<span class="keyword">const</span> CvRect* rect1, <span class="keyword">const</span> CvRect* rect2)</span></span>;</span><br><span class="line"><span class="comment">// 计算矩形的四个顶点</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvBOxPoints</span><span class="params">(CvBox2D box, CvPoint2D32f pt[<span class="number">4</span>])</span></span>;</span><br><span class="line"><span class="comment">// 从mat中初始化序列</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvPointSeqFromMat</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> seq_kind,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* mat,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvContour* contour_header,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvSeqBlock* block</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 测试一个点是否在多边形内部</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvPointPolygonTest</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* contour,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvPoint2D32f pt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> measure_dist</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="轮廓的匹配"><a href="#轮廓的匹配" class="headerlink" title="轮廓的匹配"></a>轮廓的匹配</h4><p>在实际应用中，一个跟轮廓相关的最常用到的功能是匹配两个轮廓。如果有两个轮廓，如何比较它们；或者如何比较一个轮廓和一个抽象模板。</p><ul><li><strong>矩</strong></li></ul><p>比较两个轮廓最简洁的方式是比较它们的轮廓矩。简单的说，矩是通过对轮廓上所有点进行积分运算而得到的一个粗略特征。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算轮廓矩</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvContoursMoments</span><span class="params">(CvSeq* contour, CvMoments* moments)</span></span>;</span><br><span class="line"><span class="comment">// 用于保存结果的CvMoments结构的定义</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CvMoments</span>&#123;</span></span><br><span class="line">    <span class="keyword">double</span> m00, m10, m01, m20, m11, m02, m30, m21, m12, m03;</span><br><span class="line">    <span class="keyword">double</span> mu20, mu11, mu02, mu30, mu21, mu12, mu03;</span><br><span class="line">    <span class="keyword">double</span> inv_sqrt_m00;</span><br><span class="line">&#125; CvMoments;</span><br><span class="line"><span class="comment">// 获取特定的矩</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvGetSpatialMoment</span><span class="params">(CvMoments* moments, <span class="keyword">int</span> x_order, <span class="keyword">int</span> y_order)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>再论矩</strong></li></ul><p>直接计算得到的矩并不是做比较时最好的参数。具体来说，经常会用到的是归一化的矩，因此，不同大小但是形状相同的物体会有相同的值，并且不依赖于坐标系的选择。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算图像的矩</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMoments</span><span class="params">(<span class="keyword">const</span> CvArr* <span class="built_in">image</span>, CvMoments* moments, <span class="keyword">int</span> isBinary=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="comment">// 计算中心距</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvGetCentralMoment</span><span class="params">(CvMomens* moments, <span class="keyword">int</span> x_order, <span class="keyword">int</span> y_order)</span></span>;</span><br><span class="line"><span class="comment">// 计算归一化矩</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvGetNormalizedCentralMoment</span><span class="params">(CvMomens* moments, <span class="keyword">int</span> x_order, <span class="keyword">int</span> y_order)</span></span>;</span><br><span class="line"><span class="comment">// 计算Hu不变矩</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvGetHuMoments</span><span class="params">(CvMoments* moments, CvHuMoments* HuMoments)</span></span>;</span><br></pre></td></tr></table></figure><p>Hu矩是归一化中心矩的线性组合。之所以这样做是为了能够获取代表图像某个特征的矩函数，这些矩函数对某些变化如缩放、旋转和镜像映射具有不变形。</p><ul><li><strong>使用Hu矩进行匹配</strong></li></ul><p>很自然，使用Hu矩我们想要比较两个物体并且判明它们是否相似。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 比较函数</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvMatchShapes</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span>* object1,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span>* object2,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> parameter=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>提供的物体可以是灰度图像也可以是轮廓。如果是图像，会根据指定的方法先计算矩。</p><ul><li><strong>等级匹配</strong></li></ul><p>有时，简单的相似度度量(比如矩)并不足够，所以就引出了轮廓树。轮廓树是用来描述一个特定形状内各部分的等级。编码得到的二分轮廓树包含了原始轮廓的形状信息，一旦被建立，就可以很有效的对比两个轮廓。这个过程开始定义两个树节点的对应关系，然后比较对应节点的特性。最后的结果就是两个树的相似度。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从轮廓生成轮廓树</span></span><br><span class="line"><span class="function">CvContourTree* <span class="title">cvCreateContourTree</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvSeq* contour,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> threshold</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 从轮廓树生成轮廓</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvContourFromContourTree</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvContourTree* tree,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvTermCriteria criteria</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 轮廓树匹配</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvMatchContourTrees</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvContourTree* tree1,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvContourTree* tree2,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> method,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> threshold</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>轮廓的凸包和凸缺陷</strong></li></ul><p>另一个理解物体形状或轮廓的有用的方法是计算一个物体的凸包然后计算其凸缺陷。很多复杂物体的特性能很好的被这种缺陷表现出来。<br>下面是是三个关于凸包和凸缺陷的重要函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CV_CLOCKWISE 1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CV_COUNTER_CLOCKWISE 2</span></span><br><span class="line"><span class="comment">// 计算已知轮廓的凸包</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvConvexHull2</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* input,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>* hull_storage=<span class="literal">NULL</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> orientation=CV_CLOCKWISE,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> return_points=<span class="number">0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 检查一个已知轮廓是否是凸的</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvCheckContourConvexity</span><span class="params">(<span class="keyword">const</span> CvArr* contour)</span></span>;</span><br><span class="line"><span class="comment">// 在已知轮廓是凸包的情况下计算凸缺陷</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvConvexityDefects</span><span class="params">('</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* contour,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> CvArr* convexhull,</span></span></span><br><span class="line"><span class="function"><span class="params">    CvMemStorage* storage=<span class="literal">NULL</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"><span class="comment">// 缺陷结构体的描述</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CvConvexityDefect</span>&#123;</span></span><br><span class="line">    CvPoint* start;</span><br><span class="line">    CvPoint* <span class="built_in">end</span>;</span><br><span class="line">    CvPoint* depth_point;</span><br><span class="line">    <span class="keyword">float</span> depth;</span><br><span class="line">&#125; CvConvexityDefect;</span><br></pre></td></tr></table></figure><ul><li><p><strong>成对几何直方图</strong></p><p>Freeman链码编码是对一个多边形的序列如何“移动”的描述，每个这样的移动有固定的长度和特定的方向。Freeman链码编码的用处很多，因为它支持了成对几何直方图(PGH)的基本思想。<br>PGH实际上是链码编码直方图(CCH)的一个扩展或延伸。CCH是一种直方图，用来统计一个轮廓Freeman链码编码每一种走法的数字。该直方图有旋转不变形的性质。<br>PGH的使用和FCC相似，一个重要的不同是，PGH的描述能力更强，因此在尝试解决复杂问题的时候很有用，比如说大量的形状需要被辨识，并且或者有很多背景噪声的时候。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算PGH的函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCalaPGH</span><span class="params">(<span class="keyword">const</span> CvSeq* contour, CvHistogram* hist)</span></span>;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;内存&quot;&gt;&lt;a href=&quot;#内存&quot; class=&quot;headerlink&quot; title=&quot;内存&quot;&gt;&lt;/a&gt;内存&lt;/h4&gt;&lt;p&gt;OpenCV使用内存存储器(memory storage)来统一管理各种&lt;strong&gt;动态&lt;/strong&gt;对象的内存。内存存储器在底层被实现为一个有许多相同大小的内存块组成的双向链表，通过这种结构，OpenCV可以从内存存储器中快速的分配内存或将内存返回给内存存储器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第7章 直方图与匹配</title>
    <link href="http://tramac.github.io/2018/09/15/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC7%E7%AB%A0%20%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%B8%8E%E5%8C%B9%E9%85%8D/"/>
    <id>http://tramac.github.io/2018/09/15/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC7%E7%AB%A0%20%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%B8%8E%E5%8C%B9%E9%85%8D/</id>
    <published>2018-09-15T06:24:45.000Z</published>
    <updated>2020-07-18T16:08:02.924Z</updated>
    
    <content type="html"><![CDATA[<h4 id="直方图的基本数据结构"><a href="#直方图的基本数据结构" class="headerlink" title="直方图的基本数据结构"></a>直方图的基本数据结构</h4><p>简单的说，直方图就是对数据进行统计，将统计值组织到一系列事先定义好的bin中。bin中的数值是从数据中计算出的特征的统计量，这些数据可以是诸如梯度、方向、色彩或任何其他特征。无论如何，直方图获得的是数据分布的统计图。通常直方图的维度要低于原始数据。</p><a id="more"></a><ul><li><p>直方图数据结构的定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直方图数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CvHistogram</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> type;</span><br><span class="line">    CvArr* bins;</span><br><span class="line">    <span class="keyword">float</span> thresh[CV_MAX_DIM][<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">float</span>** thresh2;</span><br><span class="line">    CvMatND mat;</span><br><span class="line">&#125;</span><br><span class="line">CvHistogram;</span><br></pre></td></tr></table></figure></li><li><p>创建新的直方图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建直方图</span></span><br><span class="line"><span class="function">cvHistogram* <span class="title">cvCreateHist</span><span class="params">(<span class="keyword">int</span> dims, <span class="keyword">int</span>* sizes, <span class="keyword">int</span> type, <span class="keyword">float</span>** ranges=<span class="literal">NULL</span>, <span class="keyword">int</span> uniform=<span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>指定直方图的ranges值：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定ranges值</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetHistBinRanges</span><span class="params">(CvHistogram* hist, <span class="keyword">float</span>** ranges, <span class="keyword">int</span> uniform=<span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>根据已给出数据创建直方图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CvHistogram* <span class="title">cvMakeHistHeaderForArray</span><span class="params">(<span class="keyword">int</span> dims, <span class="keyword">int</span>* sizes, CvHistogram* hist, <span class="keyword">float</span>* data, <span class="keyword">float</span>** ranges=<span class="literal">NULL</span>, <span class="keyword">int</span> uniform=<span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure></li></ul><h4 id="访问直方图"><a href="#访问直方图" class="headerlink" title="访问直方图"></a>访问直方图</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 访问直方图数据的函数</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvQueryHistValue_1D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvQueryHistValue_2D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvQueryHistValue_3D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1, <span class="keyword">int</span> idx2)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvQueryHistValue_nD</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span>* idxN)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">cvGetHistValue_1D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">cvGetHistValue_2D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">cvGetHistValue_3D</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1, idx2)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">cvGetHistValue_nD</span><span class="params">(CvHistogram* hist, <span class="keyword">int</span> idxN)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="直方图的基本操作"><a href="#直方图的基本操作" class="headerlink" title="直方图的基本操作"></a>直方图的基本操作</h4><ul><li><p>直方图归一化</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 归一化</span></span><br><span class="line">cvNormalizeHist(CvHistogram* hist, <span class="keyword">double</span> factor);</span><br></pre></td></tr></table></figure></li><li><p>阈值处理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 阈值函数</span></span><br><span class="line">cvThreshHist(CvHistogram* hist, <span class="keyword">double</span> factor);</span><br></pre></td></tr></table></figure></li><li><p>复制直方图</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 复制函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCopyHist</span><span class="params">(<span class="keyword">const</span> CvHistogram* src, CvHistogram** dst)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>输出直方图的最小值和最大值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvGetMinMaxHistValue</span><span class="params">(<span class="keyword">const</span> CvHistogram* hist, <span class="keyword">float</span>* min_value, <span class="keyword">float</span>* max_value, <span class="keyword">int</span>* min_idx=<span class="literal">NULL</span>, <span class="keyword">int</span>* max_idx=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>自动计算直方图</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCalcHist</span><span class="params">(IplImage** <span class="built_in">image</span>, CvHistogram* hist, <span class="keyword">int</span> accumulate=<span class="number">0</span>, <span class="keyword">const</span> CvArr* mask=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>对比两个直方图</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 相似度对比</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvCompareHist</span><span class="params">(<span class="keyword">const</span> CvHistogram* hist1, CvHistogram* hist2, <span class="keyword">int</span> method)</span></span>;</span><br></pre></td></tr></table></figure></li></ul><p>其中method有四种选择，相关(method=CV_COMP_CORREL)、卡方(method=CV_COMP_CHISQR)、直方图相交(method=CV_COMP_INTERSECT)、Bhattacharyya距离(method=CV_COMP_BHATTACHARYYA)。</p><h4 id="一些更复杂的策略"><a href="#一些更复杂的策略" class="headerlink" title="一些更复杂的策略"></a>一些更复杂的策略</h4><ul><li><p>陆地移动距离(EMD)<br>陆地移动距离是一种度量准则，它实际上度量的是怎样将一个直方图的形状转变为另一个直方图的形状。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// EMD函数</span></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">cvCalcEMD2</span><span class="params">(<span class="keyword">const</span> CvArr* signature1, <span class="keyword">const</span> CvArr* signature2, <span class="keyword">int</span> distance_type, CvDistanceFunction distance_func=<span class="literal">NULL</span>, <span class="keyword">const</span> CvArr* cost_matrix=<span class="literal">NULL</span>, CvArr* flow=<span class="literal">NULL</span>, <span class="keyword">float</span>* lower_bound=<span class="literal">NULL</span>, <span class="keyword">void</span>* userdata=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>反向投影<br>反向投影是一种记录像素点或像素块如何适应直方图模型中分布的方式。例如，我们有一个颜色直方图，可以利用反向投影在图像中找到该区域。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于像素点的反向投影函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCalcBackProject</span><span class="params">(IplImage** <span class="built_in">image</span>, CvArr* back_project, <span class="keyword">const</span> CvHistogram* hist)</span></span>;</span><br><span class="line"><span class="comment">// 基于块的反向投影函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCalcBackProjectPatch</span><span class="params">(IplImage** images, CvArr* dst, CvSize patch_size, CvHistogram* hist, <span class="keyword">int</span> method, <span class="keyword">float</span> factor)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>模板匹配<br>模板匹配不是基于直方图的，是通过在输入图像上滑动图像块对实际的图像块和输入图像进行匹配。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 模板匹配函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMatchTemplate</span><span class="params">(<span class="keyword">const</span> CvArr* <span class="built_in">image</span>, <span class="keyword">const</span> CvArr* temp1, CvArr* result, <span class="keyword">int</span> method)</span></span>;</span><br></pre></td></tr></table></figure></li></ul><p>其中method有三种可选择方法，平方差匹配法(method=CV_TM_SQDIFF)、相关匹配法(method=CV_TM_CCORR)、相关匹配法(method=CV_TM_CCOEFF)。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;直方图的基本数据结构&quot;&gt;&lt;a href=&quot;#直方图的基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;直方图的基本数据结构&quot;&gt;&lt;/a&gt;直方图的基本数据结构&lt;/h4&gt;&lt;p&gt;简单的说，直方图就是对数据进行统计，将统计值组织到一系列事先定义好的bin中。bin中的数值是从数据中计算出的特征的统计量，这些数据可以是诸如梯度、方向、色彩或任何其他特征。无论如何，直方图获得的是数据分布的统计图。通常直方图的维度要低于原始数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第6章 图像变换</title>
    <link href="http://tramac.github.io/2018/09/13/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC6%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/"/>
    <id>http://tramac.github.io/2018/09/13/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC6%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/</id>
    <published>2018-09-13T05:45:06.000Z</published>
    <updated>2020-07-18T16:07:46.195Z</updated>
    
    <content type="html"><![CDATA[<h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 卷积函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvFilter2D</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* kernel, CvPoint anchor=cvPoint(<span class="number">-1</span>, <span class="number">-1</span>))</span></span>;</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li><strong>卷积边界</strong></li></ul><p>对于卷积，存在的一个问题是如何处理卷积边界。通用的做法是，先将特定的图像轻微变大，然后以各种方式自动填充图像边界，也就是所谓的padding。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实现padding的函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCopyMakeBorder</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, CvPoint offset, <span class="keyword">int</span> bordertype, CvScalar value=cvScalarALL(<span class="number">0</span>))</span></span>;</span><br></pre></td></tr></table></figure><p>Bordertype可以是CV_BORDER_CONSTANT或者CV_BORDER_REPLICATE。</p><h4 id="梯度和Sobel导数"><a href="#梯度和Sobel导数" class="headerlink" title="梯度和Sobel导数"></a>梯度和Sobel导数</h4><p>图像处理中一个最基本的卷积运算是导数的计算。通常来说，用来表达微分的最常用的操作是Soble微分算子，Sobel算子包含任意阶的微分以及融合偏导。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sobel算子函数原型</span></span><br><span class="line">cvSobel(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> xorder, <span class="keyword">int</span> yorder, <span class="keyword">int</span> aperture_size=<span class="number">3</span>);</span><br></pre></td></tr></table></figure><ul><li><strong>Scharr滤波器</strong></li></ul><p>对于Sobel算子，当试图估计图像的方向导数时，往往不是很准确，当梯度角越接近水平或者垂直方向时，这样的不准确就更加明显。Scharr滤波器同sobel滤波器一样快，但是准确率更高，所以当利用3*3滤波器实现图像质量的时候应该使用Scharr滤波器。</p><h4 id="拉普拉斯变换"><a href="#拉普拉斯变换" class="headerlink" title="拉普拉斯变换"></a>拉普拉斯变换</h4><p>因为拉普拉斯算子可以用二次导数的形式定义，可假设其离散实现类似于二阶Sobel导数，所以OpenCV在计算拉普拉斯算子时可以直接使用Sobel算子，但是同时也提供了拉普拉斯算子。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Laplace算子</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvLaplace</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> apertureSize=<span class="number">3</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>拉普拉斯算子一个通常的应用是检测“团块”。由于拉普拉斯算子的形式是沿着X和Y轴的二次导数的和，这就意味着周围是更高值的单点或者小块会将使这个函数值最大化，反过来说，周围是更低值的点将会是函数的负值最大化。<br>拉普拉斯算子也可以用于边缘检测。</p><h4 id="Canny算子"><a href="#Canny算子" class="headerlink" title="Canny算子"></a>Canny算子</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// canny边缘检测算子</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCanny</span><span class="params">(<span class="keyword">const</span> CvArr* img, CvArr* edges, <span class="keyword">double</span> lowThresh, <span class="keyword">double</span> highThresh, <span class="keyword">int</span> apertureSize=<span class="number">3</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="霍夫变换"><a href="#霍夫变换" class="headerlink" title="霍夫变换"></a>霍夫变换</h4><p>霍夫变换是一种在图像中寻找直线，圆及其他简单形状的方法。</p><ul><li><strong>霍夫线变换</strong></li></ul><p>霍夫线变换的基本理论是二值图像中的任何两点都可能是一些候选直线集合的一部分。如果要确定每条线进行参数化，例如一个斜率a和截距b，原始图像中华的一点会变换为（a,b）平面上的轨迹，轨迹上的点对应着所有过原始图像上点的直线。<br>OpenCV支持两种不同形式的霍夫变换：标准霍夫变换(SHT)和累计概率霍夫变换(PPHT)。两者可以通过一个函数实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 霍夫变换检测直线</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvHoughLines2</span><span class="params">(CvArr* <span class="built_in">image</span>, <span class="keyword">void</span>* line_storage, <span class="keyword">int</span> method, <span class="keyword">double</span> rho, <span class="keyword">double</span> theta, <span class="keyword">int</span> threshold, <span class="keyword">double</span> param1=<span class="number">0</span>, <span class="keyword">double</span> param2=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>霍夫圆变换</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 霍夫变换检测圆</span></span><br><span class="line"><span class="function">CvSeq* <span class="title">cvHoughCircles</span><span class="params">(CvArr* <span class="built_in">image</span>, <span class="keyword">void</span>* circle_storage, <span class="keyword">int</span> method, <span class="keyword">double</span> dp, <span class="keyword">double</span> min_dist, <span class="keyword">double</span> param1=<span class="number">100</span>, <span class="keyword">double</span> param2=<span class="number">300</span>, <span class="keyword">int</span> min_radius=<span class="number">0</span>, <span class="keyword">int</span> max_dadius=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="重映射"><a href="#重映射" class="headerlink" title="重映射"></a>重映射</h4><p>在某些情况，需要把一幅图像中一个位置的像素重映射到另一个位置。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 重映射函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvRemap</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvArr* mapx, <span class="keyword">const</span> CvArr* mapy, <span class="keyword">int</span> flags=CV_INTER_LINEAR | CV_WARP_FILL_OUTLERS, CvScalar fillval=cvScalarAll(<span class="number">0</span>))</span></span>;</span><br></pre></td></tr></table></figure><h4 id="拉伸、收缩、扭曲和旋转"><a href="#拉伸、收缩、扭曲和旋转" class="headerlink" title="拉伸、收缩、扭曲和旋转"></a>拉伸、收缩、扭曲和旋转</h4><p>在一些数据扩增的技术中离不开图像的几何操作，这些几何变换包括拉伸，扭曲，旋转等。一个任意的仿射变换可以表达为乘以一个矩阵再加上一个向量的形式。<br>仿射变换可以将矩形转换为平行四边形。它可以将矩形的边压扁但必须保持边是平行的，也可以将矩形旋转或者按比例变化。而透视变换提供了更大的灵活性，一个透视变换可以将矩形变成梯形。</p><ul><li><strong>仿射变换</strong></li></ul><p>有两种情况会用到仿射变换。第一种是有一幅想要转换的图像，第二种是我们有一个点序列并想以此计算出变换。</p><p><strong>稠密仿射变换</strong><br>对于第一种情况，显然输入和输出的格式是图像，并且隐含的要求是扭曲假设对于所使用的图像，其像素必须是其稠密的表现形式。这意味着图像扭曲必须进行一些插值运算以使输出的图像平滑并且看起来自然一些。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 稠密变换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvWarpAffine</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* map_matirx, <span class="keyword">int</span> flags=CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS, CvScalar fillval=cvScalarAll(<span class="number">0</span>))</span></span>;</span><br><span class="line"><span class="comment">// 开销更小的稠密变换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvGetQuadrangeleSubPix</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* map_matrix)</span></span>;</span><br><span class="line"><span class="comment">// 仿射映射矩阵的计算</span></span><br><span class="line"><span class="function">CvMat* <span class="title">cvGetAffineTransform</span><span class="params">(<span class="keyword">const</span> CvPoint2D32f* pts_src, <span class="keyword">const</span> CvPoint2D32f* pts_dst, CvMat* map_matrix)</span></span>;</span><br><span class="line"><span class="comment">// 仿射映射矩阵的计算2</span></span><br><span class="line"><span class="function">CvMat* <span class="title">cv2DRotationMatrix</span><span class="params">(CvPoint2D32f center, <span class="keyword">double</span> angle, <span class="keyword">double</span> scale, CvMat* map_matrix)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>稀疏仿射变换</strong><br>对于稀疏映射（如，对一系列独立点的映射），一般采用以下方法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 稀疏映射函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvTransform</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* transmat, <span class="keyword">const</span> CvMat* shiftvec=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>透视变换</strong></li></ul><p><strong>密集透视变换</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 密集透视变换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvWarpPerspective</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* map_matrix, <span class="keyword">int</span> flags=CV_INTER_LINERA+CV_WARP_FILL_OUTLIERS, CvScalar fillval=cvScalarAll(<span class="number">0</span>))</span></span>;</span><br><span class="line"><span class="comment">// 透视映射矩阵map_matrix的计算</span></span><br><span class="line"><span class="function">CvMat* <span class="title">cvGetPerspectiveTransform</span><span class="params">(<span class="keyword">const</span> CvPoint2D32f* pts_src, <span class="keyword">const</span> CvPoint2D32f* pts_dst, CvMat* map_matrix)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>稀疏透视变换</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 稀疏仿射变换</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPerspectiveTransform</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">const</span> CvMat* mat)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="CartToPolar与PolarToCart"><a href="#CartToPolar与PolarToCart" class="headerlink" title="CartToPolar与PolarToCart"></a>CartToPolar与PolarToCart</h4><p>两者通常会被用于更复杂的变换之中，如cvLogPolar()。函数本身是将数值在笛卡尔空间和极性或者径向空间之间进行映射。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 笛卡尔空间映射到极性空间</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCartToPolar</span><span class="params">(<span class="keyword">const</span> CvArr* x, <span class="keyword">const</span> CvArr* y, CvArr* magnitude, CvArr* angle=<span class="literal">NULL</span>, <span class="keyword">int</span> angle_in_degrees=<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="comment">// 极性空间映射到笛卡尔空间</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPolarToCart</span><span class="params">(<span class="keyword">const</span> CvArr* magnitude, <span class="keyword">const</span> CvArr* angle, CvArr* x, CvArr* y, <span class="keyword">int</span> angle_in_degrees=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="LogPolar"><a href="#LogPolar" class="headerlink" title="LogPolar"></a>LogPolar</h4><p>对于二维图像，Log-polar转换表示从笛卡尔坐标到极坐标的变换。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对数极坐标转换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvLogPolar</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, CvPoint2D32f center, <span class="keyword">double</span> m, <span class="keyword">int</span> flags=CV_INTER_LINEAR | CV_WARP_FILL_OUTFIERS)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="离散傅里叶变换-DFT"><a href="#离散傅里叶变换-DFT" class="headerlink" title="离散傅里叶变换(DFT)"></a>离散傅里叶变换(DFT)</h4><p>函数cvDFT()可以计算输入是一维或二维数组时的FFT。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实现FFT函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDFT</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> flags, <span class="keyword">int</span> nonzero_rows=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><p><strong>频谱乘法</strong><br>在许多包含计算DFT的应用中，还必须将两个频谱中的每个元素分别相乘。由于DFT的结果是以其特殊的高密度格式封装，并且通常是复数，OpenCV通过cvMulSpectrums()函数解除它们的封装以及通过普通的矩阵操作来进行乘法运算。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将两个频谱对应元素相乘</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMulSpectrums</span><span class="params">(<span class="keyword">const</span> CvArr* src1, <span class="keyword">const</span> CvArr* src2, CvArr* dst, <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p><strong>卷积和DFT</strong><br>利用DFT可以大大加快卷积运算的速度，因为卷积定理说明空间域的卷积运算可以转换为频域的乘法运算。通常过程是，首先计算图像的傅里叶变换，然后计算卷积核的傅里叶变换，然后在变换域中以相对于图像像素数目的线性时间内进行卷积运算。</p></li></ul><h4 id="离散余弦变换-DCT"><a href="#离散余弦变换-DCT" class="headerlink" title="离散余弦变换(DCT)"></a>离散余弦变换(DCT)</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 余弦变换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDCT</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="积分图像"><a href="#积分图像" class="headerlink" title="积分图像"></a>积分图像</h4><p>积分图是一个数据结构，可实现子区域的快速求和。这样的求和在人脸识别及相关算法中应用的Haar小波变换是很用的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 求积分图像</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvIntegral</span><span class="params">(<span class="keyword">const</span> CvArr* <span class="built_in">image</span>, CvArr* sum, CvArr* sqsum=<span class="literal">NULL</span>, CvArr* tilted_sum=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="距离变换"><a href="#距离变换" class="headerlink" title="距离变换"></a>距离变换</h4><p>图像的距离变换被定义为一幅新图像，该图像的每个输出像素被设成与输入像素中0像素最近的距离。显然，典型的距离变换的输入应为某些边缘图像。在多数应用中，距离变换的输入是例如Canny边缘检测的检测图像的转换输出。<br>在实际应用中，距离变换通常是利用3x3或5x5数组掩模进行的。数组中的每个点被定义为这个特殊位置同其他相关的掩模中心的距离。较大的距离以由整个掩模定义的“动作”序列的形式被建立，这就意味着要用更大的掩模将生成更准确的距离。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 距离变换函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDistTransform</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> distance_type=CV_DIST_L2, <span class="keyword">int</span> mask_size=<span class="number">3</span>, <span class="keyword">const</span> <span class="keyword">float</span>* kernel=<span class="literal">NULL</span>, CvArr* labels=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直方图均衡化函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvEqualizeHist</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst)</span></span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;卷积&quot;&gt;&lt;a href=&quot;#卷积&quot; class=&quot;headerlink&quot; title=&quot;卷积&quot;&gt;&lt;/a&gt;卷积&lt;/h4&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 卷积函数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;cvFilter2D&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; CvArr* src, CvArr* dst, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; CvMat* kernel, CvPoint anchor=cvPoint(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;))&lt;/span&gt;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第5章 图像处理</title>
    <link href="http://tramac.github.io/2018/09/10/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC5%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <id>http://tramac.github.io/2018/09/10/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC5%E7%AB%A0%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</id>
    <published>2018-09-10T02:33:21.000Z</published>
    <updated>2020-07-18T16:07:35.272Z</updated>
    
    <content type="html"><![CDATA[<h4 id="平滑处理-模糊处理"><a href="#平滑处理-模糊处理" class="headerlink" title="平滑处理(模糊处理)"></a>平滑处理(模糊处理)</h4><p>平滑处理的用途有很多，最常见的是用来减少图像上的噪声或者失真。降低图像分辨率时，平滑处理很重要。目前OpenCV可提供五种不同的平滑操作，分别为简单模糊，简单无缩放变换的模糊，中值模糊，高斯模糊以及双边滤波。</p><a id="more"></a><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 平滑操作实现函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSmooth</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> smoothtype=CV_GAUSSIAN, <span class="keyword">int</span> param1=<span class="number">3</span>, <span class="keyword">int</span> param2=<span class="number">0</span>, <span class="keyword">double</span> param3=<span class="number">0</span>, <span class="keyword">double</span> param4=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="图像形态学"><a href="#图像形态学" class="headerlink" title="图像形态学"></a>图像形态学</h4><ul><li><strong>膨胀和腐蚀</strong></li></ul><p>膨胀是指将一些图像（或图像中的一部分区域，称之为A）与核（称之为B）进行卷积。通常情况下，核是一个小的中间带有参考点的实心正方形或圆盘。核可以视为模板或掩码，膨胀是求局部最大值的操作。核B与图像卷积，即计算核B覆盖的区域的像素点最大值，并把这个最大值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐增长。<br>腐蚀是膨胀的反操作。腐蚀操作要计算核区域像素的最小值。当核B与图像卷积时，计算被B覆盖区域的最小像素值，并把这个值放到参考点上。腐蚀能够消除细的凸起。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 膨胀操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvErode</span><span class="params">(IplImage* src, IplImage* dst, IplConvKernel* B=<span class="literal">NULL</span>, <span class="keyword">int</span> iterations=<span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="comment">// 腐蚀操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDilate</span><span class="params">(IplImage* src, IplImage* dst, IplConvKernel* B=<span class="literal">NULL</span>, <span class="keyword">int</span> iterations=<span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="comment">// 自定义核B</span></span><br><span class="line"><span class="function">IplConvKernel* <span class="title">cvCreateStructuringElementEx</span><span class="params">(<span class="keyword">int</span> cols, <span class="keyword">int</span> rows, <span class="keyword">int</span> anchor_x, <span class="keyword">int</span> anchor_y, <span class="keyword">int</span> shape, <span class="keyword">int</span>* values=<span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="comment">// 释放自定义的核</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvReleaseStructuringElement</span><span class="params">(IplConvKernel** element)</span></span>;</span><br></pre></td></tr></table></figure><p>形态核与卷积核不同，不需要任何的数值填充核，它只是标明了计算最大值或最小值的范围。</p><ul><li><strong>更通用的形态学</strong></li></ul><p>基本的腐蚀和膨胀操作通常用于处理布尔图像和图像掩码。然而，在处理灰度或彩色图像时，往往需要一些额外的操作。<br><strong>开运算：</strong>先腐蚀再膨胀，通常可以用来统计二值图像中的区域数。开运算消除高于其邻近点的孤立点。<br><strong>闭运算：</strong>先膨胀再腐蚀，对于连通区域分析，通常先采用闭运算来消除纯粹由噪声引起的部分，然后用开运算来连接邻近的区域。闭运算消除低于其邻近点的孤立点。<br><strong>形态学梯度：</strong>对二值图像，可以将团块的边缘突出出来。能够描述图像亮度变化的剧烈程度。<br><strong>礼帽和黑帽：</strong>两者分别用于分离比邻近的点亮或暗的一些斑块。当试图孤立的部分相对于其邻近的部分有亮度变化时，就可以使用这些方法。礼帽操作是从A中减去了A的开运算。黑帽操作是A的闭运算减去A。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通用的形态学操作函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMorphologyEx</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, CvArr* temp, IplConvKernel* element, <span class="keyword">int</span> operation, <span class="keyword">int</span> iterations=<span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>漫水填充法</strong></li></ul><p>漫水填充法经常被用来标记或分离图像的一部分以便对其进行进一步处理或分析；也可以用来从输入图像获取掩码区域，掩码会加速处理过程，或只处理掩码指定的像素点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 填充算法函数原型</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvFloodFill</span><span class="params">(Iplimage* img, CvPoint seedPoint, CvScalar newVal, CvScalar loDiff=cvScalarAll(<span class="number">0</span>), CvScalar upDiff=cvScalarAll(<span class="number">0</span>), CvConnectedComp* comp=<span class="number">4</span>, CvArr* mask=<span class="literal">NULL</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>尺寸调整</strong></li></ul><p>cvResize()函数用于放大或缩小图像。该函数可以将源图像精确转换为目标图像的尺寸。如果源图像中设置了ROI，那么cvResize()将会对ROI区域调整尺寸，以匹配目标图像，同样，如果目标图像中已经设置ROI的值，那么cvResize()将会源图像进行尺寸调整并填充到目标图像的ROI中。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cvRsize()函数原型</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvResize</span><span class="params">(<span class="keyword">const</span> CvArr* src, CvArr* dst, <span class="keyword">int</span> interpolation=CV_INTER_LINEAR)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>图像金字塔</strong></li></ul><p>图像金字塔是一个图像集合，集合中所有的图像都源于同一个原始图像，而且是通过对原始图像连续降采样获得，直到达到某个终止条件才停止降采样。在应用中常有两种类型的图像金字塔：高斯金字塔和拉普拉斯金字塔。高斯金字塔用来向下降采样图像，而拉普拉斯金字塔则用来从金字塔底层图像中向上采样重建一个图像。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从金字塔中上一级图像生成下一级图像，新图像面积会变为源图像的1/4</span></span><br><span class="line">void cvPyrDown(IplImage* src, IplImage* dst, IplFilter filter=CV_GAUSSIAN_5×5));</span><br><span class="line"><span class="comment">// 将现有的图像在每个维度都放大2倍</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPyrUp</span><span class="params">(IplImage* src, IplImage* dst, IplFilter filter=CV_GAUSSIAN_5×<span class="number">5</span>)</span></span>;</span><br><span class="line"><span class="comment">// 利用金字塔实现图像分割</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPySegmentation</span><span class="params">(IplImage* src, IplImage* dst, CvMemStorage* storage, CvSeg** comp, <span class="keyword">int</span> level, <span class="keyword">double</span> threshold1, <span class="keyword">double</span> threshold2)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><strong>阈值化</strong></li></ul><p>基本思想：给定一个数组和一个阈值，然后根据数组中的每个元素的值是低于还是高于阈值而进行一些处理。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvThreshold</span><span class="params">(CvArr* src, CvArr* dst, <span class="keyword">double</span> threshold, <span class="keyword">double</span> max_value, <span class="keyword">int</span> threshold_type)</span></span>;</span><br></pre></td></tr></table></figure><p>对于有很强照明或反射梯度的图像，需要根据梯度进行阈值化时，往往需要自适应阈值技术。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自适应阈值函数原型</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvAdaptiveThreshold</span><span class="params">(CvArr* src, CvArr* dst, <span class="keyword">double</span> max_val, <span class="keyword">int</span> adaptive_method=CV_ADAPTIVE_THRESH_MEAN_C, <span class="keyword">int</span> threshold_type=CV_THRESH_BINARY, <span class="keyword">int</span> block_size=<span class="number">3</span>, <span class="keyword">double</span> param1=<span class="number">5</span>)</span></span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;平滑处理-模糊处理&quot;&gt;&lt;a href=&quot;#平滑处理-模糊处理&quot; class=&quot;headerlink&quot; title=&quot;平滑处理(模糊处理)&quot;&gt;&lt;/a&gt;平滑处理(模糊处理)&lt;/h4&gt;&lt;p&gt;平滑处理的用途有很多，最常见的是用来减少图像上的噪声或者失真。降低图像分辨率时，平滑处理很重要。目前OpenCV可提供五种不同的平滑操作，分别为简单模糊，简单无缩放变换的模糊，中值模糊，高斯模糊以及双边滤波。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第4章 细说HighGUI</title>
    <link href="http://tramac.github.io/2018/09/06/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC4%E7%AB%A0%20%E7%BB%86%E8%AF%B4HighGUI/"/>
    <id>http://tramac.github.io/2018/09/06/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC4%E7%AB%A0%20%E7%BB%86%E8%AF%B4HighGUI/</id>
    <published>2018-09-06T08:06:45.000Z</published>
    <updated>2020-07-18T16:07:18.986Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建窗口"><a href="#创建窗口" class="headerlink" title="创建窗口"></a>创建窗口</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvNameWindow</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* name, <span class="keyword">int</span> flags=CV_WINDOW_AUTOSIZE)</span></span>; <span class="comment">// 创建窗口</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">cvGetWindowHandle</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* name)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">cvGetWindowName</span><span class="params">(<span class="keyword">void</span>* window_handle)</span></span>; <span class="comment">// 获取窗口名称</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvResizeWindow</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* name, <span class="keyword">int</span> <span class="built_in">width</span>, <span class="keyword">int</span> <span class="built_in">height</span>)</span></span>; <span class="comment">// 调整窗口大小</span></span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="载入图像"><a href="#载入图像" class="headerlink" title="载入图像"></a>载入图像</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">IplImage* <span class="title">cvLoadImage</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename, <span class="keyword">int</span> iscolor=CV_LOAD_IMAGE_COLOR)</span></span>; <span class="comment">// 从磁盘载入图像</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvSaveImage</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename, <span class="keyword">const</span> CvArr* <span class="built_in">image</span>)</span></span>; <span class="comment">// 保存图像</span></span><br></pre></td></tr></table></figure><h4 id="显示图像"><a href="#显示图像" class="headerlink" title="显示图像"></a>显示图像</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvShowImage</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* name, <span class="keyword">const</span> CvArr* <span class="built_in">image</span>)</span></span>; <span class="comment">// 显示图像</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvMoveWindow</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* name, <span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>; <span class="comment">// 将窗口移动到其左上角为x, y的位置</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDestroyAllWindows</span><span class="params">(<span class="keyword">void</span>)</span></span>; <span class="comment">// 释放所有窗口</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvStartWindowThread</span><span class="params">(<span class="keyword">void</span>)</span></span>; <span class="comment">// 创建一个线程用来自动更新窗口以及处理其他窗口触发事件</span></span><br></pre></td></tr></table></figure><h4 id="鼠标事件"><a href="#鼠标事件" class="headerlink" title="鼠标事件"></a>鼠标事件</h4><p>鼠标事件响应采用回调函数的方式来处理。即，为了可以响应鼠标点击事件，首先必须创建一个回调函数，使鼠标点击事件发生时，OpenCV可以调用这个函数。创建这个函数之后，需要在OpenCV中注册这个函数，以便特定窗口被触发鼠标事件以后，OpenCV可以正确调用这个函数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CvMouseCallback</span><span class="params">(<span class="keyword">int</span> event, <span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> flags, <span class="keyword">void</span>* param)</span></span>; <span class="comment">// 回调函数的定义</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetMouseCallback</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* window_name, CvMouseCallback on_mouse, <span class="keyword">void</span>* param=<span class="literal">NULL</span>)</span></span>; <span class="comment">// 注册回调函数</span></span><br></pre></td></tr></table></figure><h4 id="Sliders-Trackbars-Switches"><a href="#Sliders-Trackbars-Switches" class="headerlink" title="Sliders, Trackbars, Switches"></a>Sliders, Trackbars, Switches</h4><p>滑动条的实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 创建滑动条函数</span><br><span class="line">int cvCreateTrackbar(const char* trackbar_name, const char* window_name, int* value, int count, CvTrackbarCallback on_change);</span><br><span class="line">&#x2F;&#x2F; 读取滑动条的value值</span><br><span class="line">int cvGetTrackbarPos(const char* trackbar_name, const char* window_name);</span><br><span class="line">&#x2F;&#x2F; 设置滑动条的value值</span><br><span class="line">void cvSetTrackbarPos(const char* trackbar_name, const char* window_name, int pos);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;创建窗口&quot;&gt;&lt;a href=&quot;#创建窗口&quot; class=&quot;headerlink&quot; title=&quot;创建窗口&quot;&gt;&lt;/a&gt;创建窗口&lt;/h4&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;cvNameWindow&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* name, &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; flags=CV_WINDOW_AUTOSIZE)&lt;/span&gt;&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 创建窗口&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt;* &lt;span class=&quot;title&quot;&gt;cvGetWindowHandle&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* name)&lt;/span&gt;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* &lt;span class=&quot;title&quot;&gt;cvGetWindowName&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt;* window_handle)&lt;/span&gt;&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 获取窗口名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;cvResizeWindow&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* name, &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;width&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;height&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 调整窗口大小&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
  <entry>
    <title>第3章 初探OpenCV</title>
    <link href="http://tramac.github.io/2018/09/05/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC3%E7%AB%A0%20%E5%88%9D%E6%8E%A2OpenCV/"/>
    <id>http://tramac.github.io/2018/09/05/%E5%AD%A6%E4%B9%A0OpenCV/%E7%AC%AC3%E7%AB%A0%20%E5%88%9D%E6%8E%A2OpenCV/</id>
    <published>2018-09-05T03:45:33.000Z</published>
    <updated>2020-07-23T02:23:55.364Z</updated>
    
    <content type="html"><![CDATA[<h4 id="OpenCV的基本数据类型"><a href="#OpenCV的基本数据类型" class="headerlink" title="OpenCV的基本数据类型"></a>OpenCV的基本数据类型</h4><a id="more"></a><ul><li><strong>基本数据类型</strong></li></ul><table><thead><tr><th align="left">结构</th><th align="left">成员</th><th align="left">意义</th></tr></thead><tbody><tr><td align="left">CvPoint</td><td align="left">int x, y</td><td align="left">图像中的点</td></tr><tr><td align="left">CvPoint2D32f</td><td align="left">float x, y</td><td align="left">二维空间中的点</td></tr><tr><td align="left">CvPoint3D32f</td><td align="left">float x, y， z</td><td align="left">三维空间中的点</td></tr><tr><td align="left">CvSize</td><td align="left">int width, height</td><td align="left">图像的尺寸</td></tr><tr><td align="left">CvRect</td><td align="left">int x, y, width, height</td><td align="left">图像的部分区域</td></tr><tr><td align="left">CvScalar</td><td align="left">double val[4]</td><td align="left">RGBA值</td></tr></tbody></table><h4 id="CvMat矩阵结构"><a href="#CvMat矩阵结构" class="headerlink" title="CvMat矩阵结构"></a>CvMat矩阵结构</h4><p>在OpenCV中没有向量（vector）结构，需要时可以用列矩阵代替。<br>矩阵由宽度（width），高度（height），类型（type），行数据长度（step）和一个指向数据的指针构成。</p><ul><li><strong>矩阵头的定义</strong>：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">typeded <span class="class"><span class="keyword">struct</span> <span class="title">CvMat</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> type;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">step</span>;</span><br><span class="line">    <span class="keyword">int</span>* refcount;</span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        unchar* ptr;</span><br><span class="line">        short* s;</span><br><span class="line">        <span class="keyword">int</span>* i;</span><br><span class="line">        <span class="keyword">float</span>* fl;</span><br><span class="line">        <span class="keyword">double</span>* db;</span><br><span class="line">    &#125;data;</span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> rows;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">height</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> cols;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">width</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125; CvMat;</span><br></pre></td></tr></table></figure><ul><li><strong>矩阵的创建与释放</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CvMat* <span class="title">cvCreateMat</span><span class="params">(<span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> type)</span></span>; </span><br><span class="line"><span class="function">CvMat* <span class="title">cvCreateMatHeader</span><span class="params">(<span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> type)</span></span>; <span class="comment">//只创建CvMat结构，不分配存储空间</span></span><br><span class="line"><span class="function">CvMat* <span class="title">cvInitMatHeader</span><span class="params">(CvMat* mat, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> type, <span class="keyword">void</span>* data=<span class="literal">NULL</span>, <span class="keyword">int</span> <span class="built_in">step</span>=CV_AUTOSTEP)</span></span>; <span class="comment">//在已存在的结构上初始化CvMat矩阵头</span></span><br><span class="line"><span class="function">CvMat* <span class="title">cvCloneMat</span><span class="params">(<span class="keyword">const</span> CvMat* mat)</span></span>; <span class="comment">// 克隆</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvReleaseMat</span><span class="params">(CvMat** mat)</span></span>; <span class="comment">// 释放</span></span><br></pre></td></tr></table></figure><ul><li><strong>矩阵数据的存取</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> element_3_2 = CV_MAT_ELEM(*mat, <span class="keyword">float</span>, <span class="number">3</span>, <span class="number">2</span>);</span><br><span class="line"><span class="function">uchar* <span class="title">cvPtr2D</span><span class="params">(<span class="keyword">const</span> CvArr* arr, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1, <span class="keyword">int</span>* type=<span class="literal">NULL</span>)</span></span>; <span class="comment">// 返回指向所需元素的指针</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cvGetReal2D</span><span class="params">(<span class="keyword">const</span> CvArr* arr, <span class="keyword">int</span> indx0, <span class="keyword">int</span> idx1)</span></span>; <span class="comment">// 返回矩阵元素的实际值</span></span><br><span class="line"><span class="function">CvScalar <span class="title">cvGet2D</span><span class="params">(<span class="keyword">const</span> CvArr* arr, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1)</span></span>; <span class="comment">// 返回CvScalar类型</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetReal2D</span><span class="params">(CvArr* arr, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1, <span class="keyword">double</span> value)</span></span>; <span class="comment">// 设置值</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSet2D</span><span class="params">(CvArr* arr, <span class="keyword">int</span> idx0, <span class="keyword">int</span> idx1, CvScalar value)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="IplImage数据结构"><a href="#IplImage数据结构" class="headerlink" title="IplImage数据结构"></a>IplImage数据结构</h4><ul><li><strong>IplImage结构</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">IplImage</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> nSize;</span><br><span class="line">    <span class="keyword">int</span> ID;</span><br><span class="line">    <span class="keyword">int</span> nChannels;</span><br><span class="line">    <span class="keyword">int</span> alphaChannel;</span><br><span class="line">    <span class="keyword">int</span> depth;</span><br><span class="line">    <span class="keyword">char</span> colorModel[<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">char</span> dataOrder;</span><br><span class="line">    <span class="keyword">int</span> origin;</span><br><span class="line">    <span class="keyword">int</span> align;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">width</span>;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">height</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">IplROI</span> *<span class="title">roi</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">IplImage</span> *<span class="title">maskROI</span>;</span></span><br><span class="line">    <span class="keyword">void</span> *imageId;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">IplTileInfo</span> *<span class="title">tileInfo</span>;</span></span><br><span class="line">    <span class="keyword">int</span> imageSize;</span><br><span class="line">    <span class="keyword">char</span> *imageData;</span><br><span class="line">    <span class="keyword">int</span> widthStep;</span><br><span class="line">    <span class="keyword">int</span> BorderMode[<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">int</span> BorderConst[<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">char</span> *imageDataOrigin;</span><br><span class="line">&#125; IplImage;</span><br></pre></td></tr></table></figure><ul><li><strong>ROI的设置与重置</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvSetImageROI</span><span class="params">(IplImage* <span class="built_in">image</span>, CvRect <span class="built_in">rect</span>)</span></span>; <span class="comment">// 设置ROI</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvResetImageROI</span><span class="params">(IplImage* <span class="built_in">image</span>)</span></span>; <span class="comment">// 取消ROI</span></span><br></pre></td></tr></table></figure><h4 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h4><ul><li><strong>直线</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvLine</span><span class="params">(CvArr* <span class="built_in">array</span>, CvPoint pt1, CvPoint pt2, CvScalar color, <span class="keyword">int</span> thickness=<span class="number">1</span>, <span class="keyword">int</span> connectivity=<span class="number">8</span>)</span></span>; <span class="comment">// 直线</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvRectangle</span><span class="params">(CvArr* <span class="built_in">array</span>, CvPoint pt1, CvPoint pt2, CvScalar color, <span class="keyword">int</span> thickness=<span class="number">1</span>)</span></span>; <span class="comment">// 矩形</span></span><br></pre></td></tr></table></figure><ul><li><strong>圆形和椭圆</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCircle</span><span class="params">(CvArr* <span class="built_in">array</span>, CvPoint center, <span class="keyword">int</span> radius, CvScalar color, <span class="keyword">int</span> thickness=<span class="number">1</span>, <span class="keyword">int</span> connectivity=<span class="number">8</span>)</span></span>; <span class="comment">// 圆形</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvEllipse</span><span class="params">(CvArr* img, CvPoint center, CvSize axes, <span class="keyword">double</span> angle, <span class="keyword">double</span> start_angle, <span class="keyword">double</span> end_angle, CvScalar color, <span class="keyword">int</span> thickness=<span class="number">1</span>, <span class="keyword">int</span> line_type=<span class="number">8</span>)</span></span>; <span class="comment">// 椭圆</span></span><br></pre></td></tr></table></figure><ul><li><strong>多边形</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvFillPoly</span><span class="params">(CvArr* img, CvPoint** pts, <span class="keyword">int</span>* npts, <span class="keyword">int</span> contours, CvScalar color, <span class="keyword">int</span> line_type=<span class="number">8</span>)</span></span>; <span class="comment">// 多边形</span></span><br></pre></td></tr></table></figure><ul><li><strong>字体和文字</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvPutText</span><span class="params">(CvArr* img, <span class="keyword">const</span> <span class="keyword">char</span>* <span class="built_in">text</span>, CvPoint origin, <span class="keyword">const</span> CvFont* font, CvScalr color)</span></span>; <span class="comment">// 图像上输出文本</span></span><br></pre></td></tr></table></figure><h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h4><ul><li><strong>矩阵的存储和读取</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CvMat A = cvMat(<span class="number">5</span>, <span class="number">5</span>, CV_32F, the_matrix_data); <span class="comment">// 创建矩阵</span></span><br><span class="line">cvSave(<span class="string">"my_matrix.xml"</span>, &amp;A); <span class="comment">// 存储矩阵</span></span><br><span class="line">CvMat* A1 = (CvMat*) cvLoad(<span class="string">"my_matrix.xml"</span>); <span class="comment">// 读取矩阵</span></span><br></pre></td></tr></table></figure><ul><li><strong>xml格式文件的新建，存储与读取</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;OpenCV的基本数据类型&quot;&gt;&lt;a href=&quot;#OpenCV的基本数据类型&quot; class=&quot;headerlink&quot; title=&quot;OpenCV的基本数据类型&quot;&gt;&lt;/a&gt;OpenCV的基本数据类型&lt;/h4&gt;
    
    </summary>
    
    
      <category term="Learning OpenCV" scheme="http://tramac.github.io/categories/Learning-OpenCV/"/>
    
    
  </entry>
  
</feed>
